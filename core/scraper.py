"""
Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€Webãƒšãƒ¼ã‚¸ã®å–å¾—ã€robots.txtç¢ºèªã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€
è¨˜äº‹æœ¬æ–‡æŠ½å‡ºãªã©ã®Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from urllib.robotparser import RobotFileParser
import time
import logging
import re
from typing import Optional, Dict, List, Any

# ãƒ­ã‚¬ãƒ¼ã®å–å¾—
logger = logging.getLogger(__name__)


class WebScraper:
    """
    Webãƒšãƒ¼ã‚¸å–å¾—ãƒ»è§£æã‚¯ãƒ©ã‚¹

    robots.txtç¢ºèªã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€è¨˜äº‹æœ¬æ–‡æŠ½å‡ºæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
    è¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦Webãƒšãƒ¼ã‚¸ã‹ã‚‰é«˜å“è³ªãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã—ã€
    é©åˆ‡ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚
    """

    def __init__(self):
        """
        WebScraperã‚’åˆæœŸåŒ–

        ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šãªã©ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        é©åˆ‡ãªUser-Agentã‚’è¨­å®šã—ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ã”ã¨ã®ã‚¢ã‚¯ã‚»ã‚¹ç®¡ç†ã‚’æº–å‚™ã—ã¾ã™ã€‚
        """
        self.domain_last_access = {}  # ãƒ‰ãƒ¡ã‚¤ãƒ³ã”ã¨ã®æœ€çµ‚ã‚¢ã‚¯ã‚»ã‚¹æ™‚åˆ»
        self.rate_limit_delay = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å¾…ã¡æ™‚é–“ï¼ˆç§’ï¼‰
        self.timeout = 10  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
        self.user_agent = "Mozilla/5.0 (compatible; BookmarkToObsidian/1.0; +https://github.com/user/bookmark-to-obsidian)"

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": self.user_agent,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }
        )

        logger.info(f"ğŸŒ WebScraperåˆæœŸåŒ–å®Œäº† (User-Agent: {self.user_agent})")

    def fetch_page_content(self, url: str) -> Optional[str]:
        """
        ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å¼·åŒ–ã—ãŸãƒšãƒ¼ã‚¸å–å¾—æ©Ÿèƒ½

        æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¾ã™ã€‚
        robots.txtç¢ºèªã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€SSLè¨¼æ˜æ›¸æ¤œè¨¼ã€é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚

        Args:
            url: å–å¾—å¯¾è±¡ã®URL

        Returns:
            Optional[str]: å–å¾—ã•ã‚ŒãŸHTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰

        Raises:
            requests.exceptions.ConnectionError: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼
            requests.exceptions.Timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼
            requests.exceptions.HTTPError: HTTPã‚¨ãƒ©ãƒ¼
            requests.exceptions.SSLError: SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼
            ValueError: ç„¡åŠ¹ãªURLå½¢å¼
        """
        try:
            # URLã®è§£æ
            parsed_url = urlparse(url)
            domain = parsed_url.netloc.lower()

            logger.debug(f"ğŸŒ ãƒšãƒ¼ã‚¸å–å¾—é–‹å§‹: {url}")

            # robots.txtãƒã‚§ãƒƒã‚¯
            if not self.check_robots_txt(domain):
                logger.info(f"ğŸš« robots.txtæ‹’å¦ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—: {url}")
                return None

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®é©ç”¨
            self.apply_rate_limiting(domain)

            # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
            try:
                response = self.session.get(
                    url,
                    timeout=self.timeout,
                    allow_redirects=True,
                    verify=True,  # SSLè¨¼æ˜æ›¸æ¤œè¨¼ã‚’æœ‰åŠ¹åŒ–
                )

                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã®ç¢ºèª
                response.raise_for_status()

            except requests.exceptions.Timeout:
                logger.warning(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url} (timeout={self.timeout}s)")
                raise requests.exceptions.Timeout(
                    f"ãƒšãƒ¼ã‚¸å–å¾—ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ: {url}"
                )

            except requests.exceptions.SSLError:
                logger.warning(f"ğŸ”’ SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼: {url}")
                raise requests.exceptions.SSLError(
                    f"SSLè¨¼æ˜æ›¸ã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {url}"
                )

            except requests.exceptions.ConnectionError:
                logger.warning(f"ğŸ”Œ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {url}")
                raise requests.exceptions.ConnectionError(
                    f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {url}"
                )

            except requests.exceptions.HTTPError as e:
                status_code = e.response.status_code if e.response else "ä¸æ˜"
                logger.warning(
                    f"ğŸš« HTTPã‚¨ãƒ©ãƒ¼: {url} - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {status_code}"
                )

                # ç‰¹å®šã®HTTPã‚¨ãƒ©ãƒ¼ã«å¯¾ã™ã‚‹è©³ç´°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                if status_code == 403:
                    raise requests.exceptions.HTTPError(
                        f"ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸ (403): {url}"
                    )
                elif status_code == 404:
                    raise requests.exceptions.HTTPError(
                        f"ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (404): {url}"
                    )
                elif status_code == 429:
                    raise requests.exceptions.HTTPError(
                        f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸ (429): {url}"
                    )
                elif status_code >= 500:
                    raise requests.exceptions.HTTPError(
                        f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ ({status_code}): {url}"
                    )
                else:
                    raise requests.exceptions.HTTPError(
                        f"HTTPã‚¨ãƒ©ãƒ¼ ({status_code}): {url}"
                    )

            # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®è‡ªå‹•æ¤œå‡º
            if response.encoding is None:
                response.encoding = response.apparent_encoding

            # HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            html_content = response.text

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚µã‚¤ã‚ºã®æ¤œè¨¼
            if len(html_content) < 100:
                logger.warning(
                    f"âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¾ã™: {url} (ã‚µã‚¤ã‚º: {len(html_content)} æ–‡å­—)"
                )
                return None

            logger.debug(
                f"âœ… ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ: {url} (ã‚µã‚¤ã‚º: {len(html_content):,} æ–‡å­—)"
            )

            # æœ€çµ‚ã‚¢ã‚¯ã‚»ã‚¹æ™‚åˆ»ã‚’æ›´æ–°
            self.domain_last_access[domain] = time.time()

            return html_content

        except (
            requests.exceptions.Timeout,
            requests.exceptions.ConnectionError,
            requests.exceptions.HTTPError,
            requests.exceptions.SSLError,
        ):
            # æ—¢çŸ¥ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã¯å†ç™ºç”Ÿã•ã›ã‚‹
            raise

        except requests.exceptions.MissingSchema:
            logger.warning(f"âš ï¸ ç„¡åŠ¹ãªURLå½¢å¼: {url}")
            raise ValueError(f"ç„¡åŠ¹ãªURLå½¢å¼ã§ã™: {url}")

        except requests.exceptions.InvalidURL:
            logger.warning(f"âš ï¸ ç„¡åŠ¹ãªURL: {url}")
            raise ValueError(f"ç„¡åŠ¹ãªURLã§ã™: {url}")

        except Exception as e:
            logger.error(f"âŒ äºˆæœŸã—ãªã„ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {url} - {str(e)}")
            raise Exception(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

    def extract_article_content(self, html: str, url: str = "") -> Optional[Dict]:
        """
        HTMLã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆé«˜åº¦ãªæŠ½å‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰

        è¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦Webãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
        ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¿ã‚°ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯†åº¦ã€ä¸€èˆ¬çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ãªã©ã‚’è©¦è¡Œã—ã€
        æœ€ã‚‚å“è³ªã®é«˜ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é¸æŠã—ã¾ã™ã€‚

        Args:
            html: HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            url: å…ƒã®URLï¼ˆãƒ­ã‚°ç”¨ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ""ï¼‰

        Returns:
            Optional[Dict]: æŠ½å‡ºã•ã‚ŒãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
                - title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
                - content: è¨˜äº‹æœ¬æ–‡
                - tags: ã‚¿ã‚°ä¸€è¦§
                - metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                - quality_score: å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
                - extraction_method: ä½¿ç”¨ã—ãŸæŠ½å‡ºæ–¹æ³•
        """
        try:
            soup = BeautifulSoup(html, "html.parser")

            # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
            article_data = {
                "title": "",
                "content": "",
                "tags": [],
                "metadata": {},
                "quality_score": 0.0,
                "extraction_method": "",
            }

            # ä¸è¦ãªè¦ç´ ã‚’äº‹å‰ã«é™¤å»
            self._remove_unwanted_elements(soup)

            # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
            article_data["title"] = self._extract_title(soup, url)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            article_data["metadata"] = self._extract_metadata(soup)

            # ã‚¿ã‚°æƒ…å ±ã®æŠ½å‡º
            article_data["tags"] = self._extract_tags(soup, article_data["metadata"])

            # è¨˜äº‹æœ¬æ–‡ã®æŠ½å‡ºï¼ˆè¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦è¡Œï¼‰
            content_result = self._extract_main_content(soup, url)

            if content_result:
                article_data["content"] = content_result["content"]
                article_data["quality_score"] = content_result["quality_score"]
                article_data["extraction_method"] = content_result["method"]

                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªã®æ¤œè¨¼
                if self._validate_content_quality(article_data, url):
                    logger.debug(
                        f"âœ… è¨˜äº‹æœ¬æ–‡æŠ½å‡ºæˆåŠŸ: {url} (æ–‡å­—æ•°: {len(article_data['content'])}, å“è³ªã‚¹ã‚³ã‚¢: {article_data['quality_score']:.2f}, æ–¹æ³•: {article_data['extraction_method']})"
                    )
                    return article_data
                else:
                    logger.warning(f"âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªãŒåŸºæº–ã‚’æº€ãŸã—ã¾ã›ã‚“: {url}")
                    return None
            else:
                logger.warning(f"âš ï¸ è¨˜äº‹æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {url}")
                return None

        except Exception as e:
            logger.error(f"âŒ è¨˜äº‹æœ¬æ–‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {url} - {str(e)}")
            return None

    def check_robots_txt(self, domain: str) -> bool:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã®robots.txtã‚’ç¢ºèªã—ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒè¨±å¯ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯

        Args:
            domain: ç¢ºèªå¯¾è±¡ã®ãƒ‰ãƒ¡ã‚¤ãƒ³

        Returns:
            bool: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒè¨±å¯ã•ã‚Œã¦ã„ã‚‹å ´åˆTrue
        """
        try:
            robots_url = f"https://{domain}/robots.txt"
            logger.debug(f"ğŸ¤– robots.txtç¢ºèª: {robots_url}")

            # RobotFileParserã‚’ä½¿ç”¨ã—ã¦robots.txtã‚’è§£æ
            rp = RobotFileParser()
            rp.set_url(robots_url)

            # robots.txtã‚’èª­ã¿è¾¼ã¿ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            try:
                rp.read()

                # User-Agentã«å¯¾ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã‚’ãƒã‚§ãƒƒã‚¯
                # ä¸€èˆ¬çš„ãªã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼åã¨ã‚«ã‚¹ã‚¿ãƒ User-Agentã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
                user_agents_to_check = [
                    self.user_agent,
                    "*",  # å…¨ã¦ã®User-Agent
                    "Mozilla/5.0",  # ä¸€èˆ¬çš„ãªãƒ–ãƒ©ã‚¦ã‚¶
                ]

                for ua in user_agents_to_check:
                    if rp.can_fetch(ua, "/"):
                        logger.debug(f"âœ… robots.txtè¨±å¯: {domain} (User-Agent: {ua})")
                        return True

                logger.info(f"ğŸš« robots.txtæ‹’å¦: {domain}")
                return False

            except Exception as e:
                # robots.txtãŒå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„å ´åˆã¯è¨±å¯ã¨ã¿ãªã™
                logger.debug(
                    f"âš ï¸ robots.txtèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼ˆè¨±å¯ã¨ã—ã¦å‡¦ç†ï¼‰: {domain} - {str(e)}"
                )
                return True

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å®‰å…¨å´ã«å€’ã—ã¦è¨±å¯ã¨ã¿ãªã™
            logger.debug(
                f"âš ï¸ robots.txtãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ï¼ˆè¨±å¯ã¨ã—ã¦å‡¦ç†ï¼‰: {domain} - {str(e)}"
            )
            return True

    def apply_rate_limiting(self, domain: str) -> None:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã«å¯¾ã—ã¦ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é©ç”¨

        Args:
            domain: å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³
        """
        current_time = time.time()

        if domain in self.domain_last_access:
            time_since_last_access = current_time - self.domain_last_access[domain]

            if time_since_last_access < self.rate_limit_delay:
                sleep_time = self.rate_limit_delay - time_since_last_access
                logger.debug(f"â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¾…æ©Ÿ: {domain} ({sleep_time:.1f}ç§’)")
                time.sleep(sleep_time)

        # æœ€çµ‚ã‚¢ã‚¯ã‚»ã‚¹æ™‚åˆ»ã‚’æ›´æ–°
        self.domain_last_access[domain] = time.time()

    def group_urls_by_domain(self, urls: List[str]) -> Dict[str, List[str]]:
        """
        URLãƒªã‚¹ãƒˆã‚’ãƒ‰ãƒ¡ã‚¤ãƒ³ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–

        Args:
            urls: URLä¸€è¦§

        Returns:
            Dict[str, List[str]]: ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ã‚­ãƒ¼ã¨ã—ãŸURLä¸€è¦§
        """
        domain_groups = {}

        for url in urls:
            try:
                parsed_url = urlparse(url)
                domain = parsed_url.netloc.lower()

                if domain not in domain_groups:
                    domain_groups[domain] = []

                domain_groups[domain].append(url)

            except Exception as e:
                logger.warning(f"âš ï¸ URLè§£æã‚¨ãƒ©ãƒ¼: {url} - {str(e)}")
                continue

        logger.info(f"ğŸŒ ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—åŒ–å®Œäº†: {len(domain_groups)}å€‹ã®ãƒ‰ãƒ¡ã‚¤ãƒ³")
        for domain, domain_urls in domain_groups.items():
            logger.debug(f"  ğŸ“ {domain}: {len(domain_urls)}å€‹ã®URL")

        return domain_groups

    def set_rate_limit_delay(self, delay: float) -> None:
        """
        ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å¾…ã¡æ™‚é–“ã‚’è¨­å®š

        Args:
            delay: å¾…ã¡æ™‚é–“ï¼ˆç§’ï¼‰
        """
        self.rate_limit_delay = max(1.0, delay)  # æœ€å°1ç§’
        logger.info(f"â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®š: {self.rate_limit_delay}ç§’")

    def set_timeout(self, timeout: int) -> None:
        """
        ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š

        Args:
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰
        """
        self.timeout = max(5, timeout)  # æœ€å°5ç§’
        logger.info(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: {self.timeout}ç§’")

    def get_statistics(self) -> Dict[str, Any]:
        """
        WebScraperçµ±è¨ˆæƒ…å ±ã‚’å–å¾—

        Returns:
            Dict[str, Any]: çµ±è¨ˆæƒ…å ±
                - domains_accessed: ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸãƒ‰ãƒ¡ã‚¤ãƒ³æ•°
                - rate_limit_delay: ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¾…ã¡æ™‚é–“
                - timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“
                - user_agent: ä½¿ç”¨ä¸­ã®User-Agent
        """
        return {
            "domains_accessed": len(self.domain_last_access),
            "rate_limit_delay": self.rate_limit_delay,
            "timeout": self.timeout,
            "user_agent": self.user_agent,
        }

    # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ç¾¤

    def _remove_unwanted_elements(self, soup: BeautifulSoup) -> None:
        """
        ä¸è¦ãªè¦ç´ ã‚’é™¤å»ï¼ˆåºƒå‘Šã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆãªã©ï¼‰

        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        # é™¤å»å¯¾è±¡ã®ã‚»ãƒ¬ã‚¯ã‚¿
        unwanted_selectors = [
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«
            "script",
            "style",
            "noscript",
            # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ 
            "nav",
            "header",
            "footer",
            "aside",
            ".navigation",
            ".navbar",
            ".nav-menu",
            ".menu",
            ".breadcrumb",
            ".breadcrumbs",
            # åºƒå‘Šé–¢é€£
            ".advertisement",
            ".ads",
            ".ad",
            ".advert",
            ".google-ads",
            ".adsense",
            ".ad-container",
            '[id*="ad"]',
            '[class*="ad-"]',
            '[class*="ads-"]',
            # ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ»å…±æœ‰ãƒœã‚¿ãƒ³
            ".share-buttons",
            ".social-share",
            ".social-buttons",
            ".share",
            ".sharing",
            ".social-media",
            # ã‚³ãƒ¡ãƒ³ãƒˆãƒ»é–¢é€£è¨˜äº‹
            ".comments",
            ".comment-section",
            ".disqus",
            ".related-posts",
            ".related-articles",
            ".recommendations",
            # ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ»ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
            ".sidebar",
            ".widget",
            ".widgets",
            # ãã®ä»–ã®ä¸è¦è¦ç´ 
            ".popup",
            ".modal",
            ".overlay",
            ".newsletter",
            ".subscription",
            ".cookie-notice",
            ".cookie-banner",
            ".back-to-top",
            ".scroll-to-top",
        ]

        for selector in unwanted_selectors:
            for element in soup.select(selector):
                element.decompose()

    def _extract_title(self, soup: BeautifulSoup, url: str) -> str:
        """
        ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰

        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            url: å…ƒã®URL

        Returns:
            str: æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«
        """
        # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã®å„ªå…ˆé †ä½
        title_selectors = [
            "h1",  # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
            "title",  # HTMLã‚¿ã‚¤ãƒˆãƒ«
            '[property="og:title"]',  # Open Graphã‚¿ã‚¤ãƒˆãƒ«
            ".title",
            ".post-title",
            ".article-title",
            ".entry-title",
            ".page-title",
        ]

        for selector in title_selectors:
            elements = soup.select(selector)
            for element in elements:
                if selector == '[property="og:title"]':
                    title = element.get("content", "").strip()
                else:
                    title = element.get_text(strip=True)

                if title and len(title) > 5:  # æœ€å°æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
                    # ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    title = re.sub(r"\s+", " ", title)  # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’å˜ä¸€ã«
                    title = title.replace("\n", " ").replace("\t", " ")
                    return title[:200]  # æœ€å¤§200æ–‡å­—ã«åˆ¶é™

        # ã‚¿ã‚¤ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯URLã‹ã‚‰ç”Ÿæˆ
        parsed_url = urlparse(url)
        return f"è¨˜äº‹ - {parsed_url.netloc}"

    def _extract_metadata(self, soup: BeautifulSoup) -> Dict[str, str]:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º

        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

        Returns:
            Dict[str, str]: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        metadata = {}

        # ãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        meta_tags = soup.find_all("meta")
        for meta in meta_tags:
            name = meta.get("name", "").lower()
            property_attr = meta.get("property", "").lower()
            content = meta.get("content", "").strip()

            if content:
                # æ¨™æº–çš„ãªãƒ¡ã‚¿ã‚¿ã‚°
                if name in ["description", "keywords", "author", "robots", "viewport"]:
                    metadata[name] = content

                # Open Graphã‚¿ã‚°
                elif property_attr.startswith("og:"):
                    metadata[property_attr] = content

                # Articleã‚¿ã‚°
                elif property_attr.startswith("article:"):
                    metadata[property_attr] = content

                # Twitterã‚«ãƒ¼ãƒ‰
                elif name.startswith("twitter:"):
                    metadata[name] = content

        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆJSON-LDï¼‰ã®æŠ½å‡º
        json_ld_scripts = soup.find_all("script", type="application/ld+json")
        for script in json_ld_scripts:
            try:
                import json

                data = json.loads(script.string)
                if isinstance(data, dict):
                    if data.get("@type") in ["Article", "BlogPosting", "NewsArticle"]:
                        if "author" in data:
                            metadata["structured_author"] = str(data["author"])
                        if "datePublished" in data:
                            metadata["structured_date"] = data["datePublished"]
                        if "description" in data:
                            metadata["structured_description"] = data["description"]
            except (json.JSONDecodeError, AttributeError):
                continue

        return metadata

    def _extract_tags(self, soup: BeautifulSoup, metadata: Dict[str, str]) -> List[str]:
        """
        ã‚¿ã‚°æƒ…å ±ã‚’æŠ½å‡º

        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

        Returns:
            List[str]: æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚°ä¸€è¦§
        """
        tags = set()

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æŠ½å‡º
        keywords = metadata.get("keywords", "")
        if keywords:
            # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†å‰²
            keyword_list = [kw.strip() for kw in keywords.split(",") if kw.strip()]
            tags.update(keyword_list)

        # HTMLã‹ã‚‰ã‚¿ã‚°è¦ç´ ã‚’æŠ½å‡º
        tag_selectors = [
            ".tags a",
            ".tag a",
            ".categories a",
            ".category a",
            ".labels a",
            ".label a",
            ".topics a",
            ".topic a",
            '[rel="tag"]',
            ".post-tags a",
            ".entry-tags a",
        ]

        for selector in tag_selectors:
            elements = soup.select(selector)
            for element in elements:
                tag_text = element.get_text(strip=True)
                if tag_text and len(tag_text) <= 50:  # æœ€å¤§50æ–‡å­—ã®ã‚¿ã‚°ã®ã¿
                    # ã‚¿ã‚°ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    tag_text = re.sub(r"[^\w\s\-_]", "", tag_text)  # ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»
                    tag_text = re.sub(
                        r"\s+", "-", tag_text.strip()
                    )  # ã‚¹ãƒšãƒ¼ã‚¹ã‚’ãƒã‚¤ãƒ•ãƒ³ã«
                    if tag_text:
                        tags.add(tag_text)

        # ã‚¿ã‚°æ•°ã‚’åˆ¶é™ï¼ˆæœ€å¤§20å€‹ï¼‰
        return list(tags)[:20]

    def _extract_main_content(self, soup: BeautifulSoup, url: str) -> Optional[Dict]:
        """
        ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦è¡Œï¼‰

        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            url: å…ƒã®URL

        Returns:
            Optional[Dict]: æŠ½å‡ºçµæœï¼ˆcontent, quality_score, methodï¼‰
        """
        extraction_methods = [
            ("semantic_tags", self._extract_by_semantic_tags),
            ("content_density", self._extract_by_content_density),
            ("common_selectors", self._extract_by_common_selectors),
            ("body_fallback", self._extract_by_body_fallback),
        ]

        best_result = None
        best_score = 0.0

        for method_name, method_func in extraction_methods:
            try:
                result = method_func(soup)
                if result and result["quality_score"] > best_score:
                    best_result = result
                    best_score = result["quality_score"]
                    best_result["method"] = method_name

                    # ååˆ†ã«é«˜å“è³ªãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯æ—©æœŸçµ‚äº†
                    if best_score >= 0.8:
                        break

            except Exception as e:
                logger.debug(f"æŠ½å‡ºæ–¹æ³• {method_name} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue

        return best_result

    def _extract_by_semantic_tags(self, soup: BeautifulSoup) -> Optional[Dict]:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¿ã‚°ã‚’ä½¿ç”¨ã—ãŸæŠ½å‡º"""
        semantic_selectors = ["article", "main", '[role="main"]']

        for selector in semantic_selectors:
            elements = soup.select(selector)
            if elements:
                # æœ€ã‚‚é•·ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é¸æŠ
                best_element = max(elements, key=lambda x: len(x.get_text()))
                content = self._clean_content(best_element.get_text())

                if len(content) > 50:  # é–¾å€¤ã‚’ä¸‹ã’ã‚‹
                    return {
                        "content": content,
                        "quality_score": 0.9,  # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¿ã‚°ã¯é«˜å“è³ª
                        "method": "semantic_tags",
                    }

        return None

    def _extract_by_content_density(self, soup: BeautifulSoup) -> Optional[Dict]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯†åº¦ã«ã‚ˆã‚‹æŠ½å‡º"""
        # å„è¦ç´ ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯†åº¦ã‚’è¨ˆç®—
        candidates = []

        for element in soup.find_all(["div", "section", "article"]):
            text = element.get_text(strip=True)
            if len(text) < 50:  # é–¾å€¤ã‚’ä¸‹ã’ã‚‹
                continue

            # ãƒªãƒ³ã‚¯å¯†åº¦ã‚’è¨ˆç®—ï¼ˆãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆ / å…¨ãƒ†ã‚­ã‚¹ãƒˆï¼‰
            link_text = "".join([a.get_text() for a in element.find_all("a")])
            link_density = len(link_text) / len(text) if text else 1.0

            # æ®µè½æ•°ã‚’è¨ˆç®—
            paragraphs = len(element.find_all("p"))

            # å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            quality_score = (
                min(len(text) / 500, 1.0) * 0.4  # æ–‡å­—æ•°ï¼ˆæœ€å¤§500æ–‡å­—ã§1.0ï¼‰
                + (1.0 - link_density) * 0.4  # ãƒªãƒ³ã‚¯å¯†åº¦ãŒä½ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
                + min(paragraphs / 3, 1.0) * 0.2  # æ®µè½æ•°ï¼ˆæœ€å¤§3æ®µè½ã§1.0ï¼‰
            )

            candidates.append(
                {"element": element, "text": text, "quality_score": quality_score}
            )

        if candidates:
            # æœ€é«˜ã‚¹ã‚³ã‚¢ã®è¦ç´ ã‚’é¸æŠ
            best_candidate = max(candidates, key=lambda x: x["quality_score"])
            content = self._clean_content(best_candidate["text"])

            return {
                "content": content,
                "quality_score": best_candidate["quality_score"],
                "method": "content_density",
            }

        return None

    def _extract_by_common_selectors(self, soup: BeautifulSoup) -> Optional[Dict]:
        """ä¸€èˆ¬çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ã«ã‚ˆã‚‹æŠ½å‡º"""
        common_selectors = [
            ".content",
            ".post-content",
            ".entry-content",
            ".article-content",
            "#content",
            "#main-content",
            ".main-content",
            ".post-body",
            ".entry-body",
            ".article-body",
            ".content-body",
        ]

        for selector in common_selectors:
            elements = soup.select(selector)
            if elements:
                best_element = max(elements, key=lambda x: len(x.get_text()))
                content = self._clean_content(best_element.get_text())

                if len(content) > 100:
                    return {
                        "content": content,
                        "quality_score": 0.7,  # ä¸­ç¨‹åº¦ã®å“è³ª
                        "method": "common_selectors",
                    }

        return None

    def _extract_by_body_fallback(self, soup: BeautifulSoup) -> Optional[Dict]:
        """bodyã‚¿ã‚°ã‹ã‚‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æŠ½å‡º"""
        body = soup.find("body")
        if body:
            content = self._clean_content(body.get_text())

            if len(content) > 200:
                return {
                    "content": content,
                    "quality_score": 0.3,  # ä½å“è³ªï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    "method": "body_fallback",
                }

        return None

    def _clean_content(self, text: str) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

        Args:
            text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            str: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not text:
            return ""

        # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’å˜ä¸€ã®ã‚¹ãƒšãƒ¼ã‚¹ã«
        text = re.sub(r"[ \t]+", " ", text)

        # è¡Œã”ã¨ã«å‡¦ç†
        lines = []
        for line in text.split("\n"):
            line = line.strip()
            if line and len(line) > 3:  # çŸ­ã™ãã‚‹è¡Œã¯é™¤å¤–
                lines.append(line)

        # æ®µè½ã¨ã—ã¦çµåˆ
        content = "\n\n".join(lines)

        # æœ€å¤§æ–‡å­—æ•°åˆ¶é™ï¼ˆ10,000æ–‡å­—ï¼‰
        if len(content) > 10000:
            content = content[:10000] + "..."

        return content

    def _validate_content_quality(self, article_data: Dict, url: str) -> bool:
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªã‚’æ¤œè¨¼

        Args:
            article_data: è¨˜äº‹ãƒ‡ãƒ¼ã‚¿
            url: å…ƒã®URL

        Returns:
            bool: å“è³ªåŸºæº–ã‚’æº€ãŸã™å ´åˆTrue
        """
        content = article_data.get("content", "")
        quality_score = article_data.get("quality_score", 0.0)

        # åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
        checks = {
            "min_length": len(content) >= 100,  # æœ€å°100æ–‡å­—
            "max_length": len(content) <= 50000,  # æœ€å¤§50,000æ–‡å­—
            "quality_score": quality_score >= 0.3,  # æœ€å°å“è³ªã‚¹ã‚³ã‚¢
            "has_title": bool(article_data.get("title", "").strip()),  # ã‚¿ã‚¤ãƒˆãƒ«å­˜åœ¨
            "reasonable_structure": content.count("\n") >= 2,  # æœ€ä½é™ã®æ§‹é€ 
        }

        # ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ã‚’ãƒ‘ã‚¹
        passed_checks = sum(checks.values())
        total_checks = len(checks)

        success_rate = passed_checks / total_checks

        if success_rate < 0.8:  # 80%ä»¥ä¸Šã®ãƒã‚§ãƒƒã‚¯ã‚’ãƒ‘ã‚¹ã™ã‚‹å¿…è¦
            logger.debug(f"å“è³ªãƒã‚§ãƒƒã‚¯å¤±æ•—: {url} - {checks}")
            return False

        # ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãªã©ï¼‰
        error_patterns = [
            r"404.*not found",
            r"page not found",
            r"access denied",
            r"forbidden",
            r"error occurred",
        ]

        content_lower = content.lower()
        for pattern in error_patterns:
            if re.search(pattern, content_lower):
                logger.debug(f"ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º: {url} - {pattern}")
                return False

        return True
