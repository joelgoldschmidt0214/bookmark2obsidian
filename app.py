"""
Bookmark to Obsidian Converter
Streamlitãƒ™ãƒ¼ã‚¹ã®ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
Google Chromeã®bookmarks.htmlãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã€Obsidianç”¨ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
"""

import streamlit as st
from pathlib import Path
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, field
from enum import Enum
import datetime
import os
import re
import logging
import time
import requests
from urllib.parse import urlparse
from urllib.robotparser import RobotFileParser
from bs4 import BeautifulSoup

# Task 10: å¼·åŒ–ã•ã‚ŒãŸãƒ­ã‚°è¨­å®šã¨ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¨˜éŒ²æ©Ÿèƒ½
# ç’°å¢ƒå¤‰æ•°DEBUG=1ã‚’è¨­å®šã™ã‚‹ã¨ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚‚è¡¨ç¤º
log_level = logging.DEBUG if os.getenv('DEBUG') == '1' else logging.INFO

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
log_directory = Path("logs")
log_directory.mkdir(exist_ok=True)

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ—¥ä»˜ä»˜ãï¼‰
log_filename = log_directory / f"bookmark2obsidian_{datetime.datetime.now().strftime('%Y%m%d')}.log"

# ãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®š
handlers = [
    logging.StreamHandler(),  # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
    logging.FileHandler(log_filename, encoding='utf-8')  # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
]

logging.basicConfig(
    level=log_level,
    format='%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',
    handlers=handlers
)
logger = logging.getLogger(__name__)

logger.info(f"ğŸš€ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«: {logging.getLevelName(log_level)})")
logger.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_filename}")


class ErrorLogger:
    """Task 10: ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¨˜éŒ²ã¨ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.errors = []
        self.error_counts = {
            'network': 0,
            'timeout': 0,
            'fetch': 0,
            'extraction': 0,
            'markdown': 0,
            'permission': 0,
            'filesystem': 0,
            'save': 0,
            'unexpected': 0
        }
    
    def log_error(self, bookmark: 'Bookmark', error_msg: str, error_type: str, retryable: bool = False):
        """
        ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
        
        Args:
            bookmark: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯
            error_msg: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            error_type: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—
            retryable: ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ã‹ã©ã†ã‹
        """
        error_entry = {
            'timestamp': datetime.datetime.now(),
            'bookmark': bookmark,
            'error': error_msg,
            'type': error_type,
            'retryable': retryable,
            'url': bookmark.url,
            'title': bookmark.title
        }
        
        self.errors.append(error_entry)
        
        if error_type in self.error_counts:
            self.error_counts[error_type] += 1
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚è¨˜éŒ²
        logger.error(f"[{error_type.upper()}] {bookmark.title} - {error_msg}")
    
    def get_error_summary(self) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        return {
            'total_errors': len(self.errors),
            'error_counts': self.error_counts.copy(),
            'retryable_count': sum(1 for error in self.errors if error['retryable']),
            'recent_errors': self.errors[-10:] if self.errors else []
        }
    
    def get_retryable_errors(self) -> List[Dict]:
        """ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ã‚’å–å¾—"""
        return [error for error in self.errors if error['retryable']]
    
    def clear_errors(self):
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ã‚¯ãƒªã‚¢"""
        self.errors.clear()
        self.error_counts = {key: 0 for key in self.error_counts}


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
error_logger = ErrorLogger()


# ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å®šç¾©
class PageStatus(Enum):
    PENDING = "pending"
    FETCHING = "fetching"
    SUCCESS = "success"
    EXCLUDED = "excluded"
    ERROR = "error"


@dataclass
class Bookmark:
    title: str
    url: str
    folder_path: List[str]
    add_date: Optional[datetime.datetime] = None
    icon: Optional[str] = None


@dataclass
class Page:
    bookmark: Bookmark
    content: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    metadata: Dict = field(default_factory=dict)
    is_selected: bool = True
    status: PageStatus = PageStatus.PENDING


class LocalDirectoryManager:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ§‹é€ ã‚’è§£æã—ã€é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, base_path: Path):
        """
        LocalDirectoryManagerã‚’åˆæœŸåŒ–
        
        Args:
            base_path: åŸºæº–ã¨ãªã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        """
        self.base_path = Path(base_path)
        self.existing_structure = {}
        self.duplicate_files = set()
    
    def scan_directory(self, path: Optional[str] = None) -> Dict[str, List[str]]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã‚’èª­ã¿å–ã‚‹
        
        Args:
            path: ã‚¹ã‚­ãƒ£ãƒ³å¯¾è±¡ã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯base_pathã‚’ä½¿ç”¨ï¼‰
            
        Returns:
            Dict[str, List[str]]: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ã‚­ãƒ¼ã¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åä¸€è¦§
        """
        scan_path = Path(path) if path else self.base_path
        
        if not scan_path.exists() or not scan_path.is_dir():
            return {}
        
        structure = {}
        
        try:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å†å¸°çš„ã«ã‚¹ã‚­ãƒ£ãƒ³
            for root, dirs, files in os.walk(scan_path):
                # ç›¸å¯¾ãƒ‘ã‚¹ã‚’è¨ˆç®—
                relative_root = Path(root).relative_to(scan_path)
                relative_path = str(relative_root) if str(relative_root) != '.' else ''
                
                # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
                markdown_files = [
                    Path(f).stem for f in files 
                    if f.lower().endswith(('.md', '.markdown'))
                ]
                
                if markdown_files:
                    structure[relative_path] = markdown_files
            
            self.existing_structure = structure
            return structure
            
        except Exception as e:
            raise RuntimeError(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¹ã‚­ãƒ£ãƒ³ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def check_file_exists(self, path: str, filename: str) -> bool:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            path: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
            filename: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
            
        Returns:
            bool: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆTrue
        """
        import logging
        logger = logging.getLogger(__name__)
        
        try:
            # ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–
            normalized_path = path.replace('\\', '/') if path else ''
            
            logger.debug(f"    ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯: ãƒ‘ã‚¹='{normalized_path}', ãƒ•ã‚¡ã‚¤ãƒ«å='{filename}'")
            logger.debug(f"    æ—¢å­˜æ§‹é€ : {self.existing_structure}")
            
            # æ—¢å­˜æ§‹é€ ã‹ã‚‰ç¢ºèª
            if normalized_path in self.existing_structure:
                exists_in_structure = filename in self.existing_structure[normalized_path]
                logger.debug(f"    æ§‹é€ å†…ãƒã‚§ãƒƒã‚¯çµæœ: {exists_in_structure}")
                if exists_in_structure:
                    return True
            
            # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã‚‚ç¢ºèª
            full_path = self.base_path / path if path else self.base_path
            if full_path.exists():
                md_file = full_path / f"{filename}.md"
                markdown_file = full_path / f"{filename}.markdown"
                file_exists = md_file.exists() or markdown_file.exists()
                logger.debug(f"    ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯: {md_file} â†’ {md_file.exists()}")
                logger.debug(f"    ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯: {markdown_file} â†’ {markdown_file.exists()}")
                return file_exists
            
            logger.debug(f"    çµæœ: ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ã—ãªã„")
            return False
            
        except Exception as e:
            logger.error(f"    ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def compare_with_bookmarks(self, bookmarks: List[Bookmark]) -> Dict[str, List[str]]:
        """
        ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯éšå±¤ã¨æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’æ¯”è¼ƒã—ã€é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
        
        Args:
            bookmarks: ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä¸€è¦§
            
        Returns:
            Dict[str, List[str]]: é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±
        """
        import logging
        logger = logging.getLogger(__name__)
        
        duplicates = {
            'files': [],  # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
            'paths': []   # é‡è¤‡ãƒ‘ã‚¹ä¸€è¦§
        }
        
        self.duplicate_files.clear()
        
        logger.info(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯å¯¾è±¡: {len(bookmarks)}å€‹ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯")
        
        for i, bookmark in enumerate(bookmarks):
            # ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            folder_path = '/'.join(bookmark.folder_path) if bookmark.folder_path else ''
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆBookmarkParserã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            filename = self._sanitize_filename(bookmark.title)
            
            logger.debug(f"  {i+1}. ãƒã‚§ãƒƒã‚¯ä¸­: '{bookmark.title}' â†’ '{filename}' (ãƒ‘ã‚¹: '{folder_path}')")
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            file_exists = self.check_file_exists(folder_path, filename)
            logger.debug(f"     ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯çµæœ: {file_exists}")
            
            if file_exists:
                duplicate_info = f"{folder_path}/{filename}" if folder_path else filename
                duplicates['files'].append(duplicate_info)
                duplicates['paths'].append(folder_path)
                
                # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ãƒƒãƒˆã«è¿½åŠ 
                self.duplicate_files.add((folder_path, filename))
                logger.info(f"  ğŸ”„ é‡è¤‡æ¤œå‡º: {duplicate_info}")
        
        logger.info(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯å®Œäº†: {len(duplicates['files'])}å€‹ã®é‡è¤‡ã‚’æ¤œå‡º")
        return duplicates
    
    def is_duplicate(self, bookmark: Bookmark) -> bool:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãŒé‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        
        Args:
            bookmark: åˆ¤å®šå¯¾è±¡ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯
            
        Returns:
            bool: é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆTrue
        """
        folder_path = '/'.join(bookmark.folder_path) if bookmark.folder_path else ''
        filename = self._sanitize_filename(bookmark.title)
        
        return (folder_path, filename) in self.duplicate_files
    
    def get_duplicate_count(self) -> int:
        """
        é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’å–å¾—
        
        Returns:
            int: é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        """
        return len(self.duplicate_files)
    
    def create_directory_structure(self, base_path: str, structure: Dict) -> None:
        """
        ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’è‡ªå‹•ä½œæˆ
        
        Args:
            base_path: åŸºæº–ãƒ‘ã‚¹
            structure: ä½œæˆã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
        """
        try:
            base = Path(base_path)
            
            for folder_path in structure.keys():
                if folder_path:  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆ
                    full_path = base / folder_path
                    full_path.mkdir(parents=True, exist_ok=True)
                    
        except Exception as e:
            raise RuntimeError(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def save_markdown_file(self, path: str, content: str) -> bool:
        """
        Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        
        Args:
            path: ä¿å­˜å…ˆãƒ‘ã‚¹ï¼ˆbase_pathã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼‰
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
            
        Returns:
            bool: ä¿å­˜æˆåŠŸã®å ´åˆTrue
        """
        try:
            full_path = self.base_path / path
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            full_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            return True
            
        except Exception as e:
            raise RuntimeError(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _sanitize_filename(self, title: str) -> str:
        """
        ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆBookmarkParserã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        
        Args:
            title: å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«
            
        Returns:
            str: å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«å
        """
        # å±é™ºãªæ–‡å­—ã‚’é™¤å»ãƒ»ç½®æ›ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã¯ä¿æŒï¼‰
        filename = re.sub(r'[<>:"/\\|?*]', '_', title)
        
        # é€£ç¶šã™ã‚‹ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’å˜ä¸€ã«
        filename = re.sub(r'_+', '_', filename)
        
        # å‰å¾Œã®ç©ºç™½ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’é™¤å»
        filename = filename.strip(' _')
        
        # ç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåã‚’ä½¿ç”¨
        if not filename:
            filename = 'untitled'
        
        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ï¼ˆæ‹¡å¼µå­ã‚’è€ƒæ…®ã—ã¦200æ–‡å­—ä»¥å†…ï¼‰
        if len(filename) > 200:
            filename = filename[:200]
        
        return filename
    
    def get_statistics(self) -> Dict[str, int]:
        """
        ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        
        Returns:
            Dict[str, int]: çµ±è¨ˆæƒ…å ±
        """
        total_files = sum(len(files) for files in self.existing_structure.values())
        total_directories = len(self.existing_structure)
        
        return {
            'total_files': total_files,
            'total_directories': total_directories,
            'duplicate_files': len(self.duplicate_files)
        }


class BookmarkParser:
    """
    bookmarks.htmlãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self):
        self.excluded_domains = set()
        self.excluded_urls = set()
    
    def parse_bookmarks(self, html_content: str) -> List[Bookmark]:
        """
        HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä¸€è¦§ã‚’æŠ½å‡ºã™ã‚‹
        
        Args:
            html_content: bookmarks.htmlã®å†…å®¹
            
        Returns:
            List[Bookmark]: æŠ½å‡ºã•ã‚ŒãŸãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä¸€è¦§
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            bookmarks = []
            
            # ãƒ«ãƒ¼ãƒˆDLã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã‹ã‚‰é–‹å§‹
            root_dl = soup.find('dl')
            if root_dl:
                bookmarks = self._parse_dl_element(root_dl, [])
            
            return bookmarks
            
        except Exception as e:
            raise ValueError(f"ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _parse_dl_element(self, dl_element, current_path: List[str], processed_dls=None) -> List[Bookmark]:
        """
        DLã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã‚’æ„šç›´ã«è§£æã—ã¦ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚’æŠ½å‡º
        
        Args:
            dl_element: BeautifulSoupã®DLã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆ
            current_path: ç¾åœ¨ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹
            processed_dls: å‡¦ç†æ¸ˆã¿ã®DLã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®ã‚»ãƒƒãƒˆ
            
        Returns:
            List[Bookmark]: æŠ½å‡ºã•ã‚ŒãŸãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä¸€è¦§
        """
        if processed_dls is None:
            processed_dls = set()
        
        # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if id(dl_element) in processed_dls:
            return []
        
        # å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
        processed_dls.add(id(dl_element))
        
        bookmarks = []
        
        # DLã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆå†…ã®DTã‚’å‡¦ç†ï¼ˆPã‚¿ã‚°å†…ã«ã‚ã‚‹å ´åˆã‚‚è€ƒæ…®ï¼‰
        # ã¾ãšã€ã“ã®DLãƒ¬ãƒ™ãƒ«ã®DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        all_dt_in_dl = dl_element.find_all('dt')
        
        # ã“ã®DLã®ç›´æ¥ã®å­DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®ã¿ã‚’å–å¾—ï¼ˆpã‚¿ã‚°å†…ã‚‚å«ã‚€ï¼‰
        direct_dt_elements = []
        for child in dl_element.children:
            if hasattr(child, 'name'):
                if child.name == 'dt':
                    direct_dt_elements.append(child)
                elif child.name == 'p':
                    # Pè¦ç´ å†…ã®ã™ã¹ã¦ã®DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ï¼ˆãƒã‚¹ãƒˆã—ãŸDLå†…ã®ã‚‚ã®ã¯é™¤ãï¼‰
                    all_p_dts = child.find_all('dt')
                    
                    # ãƒã‚¹ãƒˆã—ãŸDLå†…ã®DTã‚’é™¤å¤–
                    nested_dls_in_p = child.find_all('dl')
                    nested_dt_in_p = set()
                    for nested_dl in nested_dls_in_p:
                        nested_dt_in_p.update(nested_dl.find_all('dt'))
                    
                    p_dt_elements = [dt for dt in all_p_dts if dt not in nested_dt_in_p]
                    direct_dt_elements.extend(p_dt_elements)
        
        for dt in direct_dt_elements:
            # DTã®æ¬¡ã®å…„å¼Ÿè¦ç´ ãŒDDã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
            next_sibling = dt.find_next_sibling()
            
            if next_sibling and next_sibling.name == 'dd':
                # DTã®å¾Œã«DDãŒã‚ã‚‹å ´åˆ â†’ ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ 
                h3 = dt.find('h3')
                if h3:
                    folder_name = h3.get_text(strip=True)
                    new_path = current_path + [folder_name]
                    
                    # DDå†…ã®DLã‚’å†å¸°çš„ã«å‡¦ç†
                    nested_dl = next_sibling.find('dl')
                    if nested_dl:
                        nested_bookmarks = self._parse_dl_element(nested_dl, new_path, processed_dls)
                        bookmarks.extend(nested_bookmarks)
            else:
                # DTã®å¾Œã«DDãŒãªã„å ´åˆã®å‡¦ç†
                # H3ã‚¿ã‚°ãŒã‚ã‚Šã€å†…éƒ¨ã«DLãŒã‚ã‚‹å ´åˆã¯ãƒ•ã‚©ãƒ«ãƒ€ã¨ã—ã¦å‡¦ç†
                h3 = dt.find('h3')
                internal_dl = dt.find('dl')
                
                if h3 and internal_dl:
                    # DTã®å†…éƒ¨ã«ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ãŒã‚ã‚‹å ´åˆï¼ˆãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãƒãƒ¼ãªã©ï¼‰
                    folder_name = h3.get_text(strip=True)
                    new_path = current_path + [folder_name]
                    nested_bookmarks = self._parse_dl_element(internal_dl, new_path, processed_dls)
                    bookmarks.extend(nested_bookmarks)
                else:
                    # é€šå¸¸ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯
                    a_tag = dt.find('a')
                    if a_tag:
                        bookmark = self._extract_bookmark_from_a_tag(a_tag, current_path)
                        if bookmark and not self._should_exclude_bookmark(bookmark):
                            bookmarks.append(bookmark)
        
        return bookmarks
    

    
    def _extract_bookmark_from_a_tag(self, a_tag, folder_path: List[str]) -> Optional[Bookmark]:
        """
        Aã‚¿ã‚°ã‹ã‚‰ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            a_tag: BeautifulSoupã®Aã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆ
            folder_path: ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹
            
        Returns:
            Optional[Bookmark]: æŠ½å‡ºã•ã‚ŒãŸãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ï¼ˆé™¤å¤–å¯¾è±¡ã®å ´åˆã¯Noneï¼‰
        """
        try:
            url = a_tag.get('href', '').strip()
            title = a_tag.get_text(strip=True)
            
            if not url or not title:
                return None
            
            # æ—¥ä»˜ã®è§£æï¼ˆADD_DATEå±æ€§ï¼‰
            add_date = None
            add_date_str = a_tag.get('add_date')
            if add_date_str:
                try:
                    # Unix timestampã‹ã‚‰å¤‰æ›
                    add_date = datetime.datetime.fromtimestamp(int(add_date_str))
                except (ValueError, TypeError):
                    pass
            
            # ã‚¢ã‚¤ã‚³ãƒ³æƒ…å ±ã®å–å¾—
            icon = a_tag.get('icon')
            
            return Bookmark(
                title=title,
                url=url,
                folder_path=folder_path,
                add_date=add_date,
                icon=icon
            )
            
        except Exception as e:
            # å€‹åˆ¥ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æã‚¨ãƒ©ãƒ¼ã¯è­¦å‘Šãƒ¬ãƒ™ãƒ«ã§å‡¦ç†
            return None
    
    def _should_exclude_bookmark(self, bookmark: Bookmark) -> bool:
        """
        ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚’é™¤å¤–ã™ã¹ãã‹ã©ã†ã‹ã‚’åˆ¤å®š
        
        Args:
            bookmark: åˆ¤å®šå¯¾è±¡ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯
            
        Returns:
            bool: é™¤å¤–ã™ã¹ãå ´åˆTrue
        """
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒˆURLã®é™¤å¤–
        if self._is_domain_root_url(bookmark.url):
            return True
        
        # ç„¡åŠ¹ãªURLã®é™¤å¤–
        if not self._is_valid_url(bookmark.url):
            return True
        
        # é™¤å¤–ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹URLã®é™¤å¤–
        if bookmark.url in self.excluded_urls:
            return True
        
        # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç¢ºèª
        try:
            parsed_url = urlparse(bookmark.url)
            domain = parsed_url.netloc.lower()
            if domain in self.excluded_domains:
                return True
        except Exception:
            return True
        
        return False
    
    def _is_domain_root_url(self, url: str) -> bool:
        """
        URLãŒãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ«ãƒ¼ãƒˆã‹ã©ã†ã‹ã‚’åˆ¤å®š
        
        Args:
            url: åˆ¤å®šå¯¾è±¡ã®URL
            
        Returns:
            bool: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒˆã®å ´åˆTrue
        """
        try:
            parsed = urlparse(url)
            # ãƒ‘ã‚¹ãŒç©ºã€ã¾ãŸã¯ã€Œ/ã€ã®ã¿ã®å ´åˆã¯ãƒ«ãƒ¼ãƒˆã¨åˆ¤å®š
            path = parsed.path.strip('/')
            is_root = len(path) == 0 and not parsed.query and not parsed.fragment
            return is_root
        except Exception:
            return False
    
    def _is_valid_url(self, url: str) -> bool:
        """
        URLãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        
        Args:
            url: åˆ¤å®šå¯¾è±¡ã®URL
            
        Returns:
            bool: æœ‰åŠ¹ãªURLã®å ´åˆTrue
        """
        try:
            parsed = urlparse(url)
            # ã‚¹ã‚­ãƒ¼ãƒ ã¨ãƒãƒƒãƒˆãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            return bool(parsed.scheme and parsed.netloc)
        except Exception:
            return False
    
    def extract_directory_structure(self, bookmarks: List[Bookmark]) -> Dict[str, List[str]]:
        """
        ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‹ã‚‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’æŠ½å‡º
        
        Args:
            bookmarks: ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä¸€è¦§
            
        Returns:
            Dict[str, List[str]]: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ã‚­ãƒ¼ã¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åä¸€è¦§
        """
        structure = {}
        
        for bookmark in bookmarks:
            # ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            folder_path = '/'.join(bookmark.folder_path) if bookmark.folder_path else ''
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œæˆï¼‰
            filename = self._sanitize_filename(bookmark.title)
            
            if folder_path not in structure:
                structure[folder_path] = []
            
            structure[folder_path].append(filename)
        
        return structure
    
    def _sanitize_filename(self, title: str) -> str:
        """
        ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        
        Args:
            title: å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«
            
        Returns:
            str: å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«å
        """
        # å±é™ºãªæ–‡å­—ã‚’é™¤å»ãƒ»ç½®æ›ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã¯ä¿æŒï¼‰
        filename = re.sub(r'[<>:"/\\|?*]', '_', title)
        
        # é€£ç¶šã™ã‚‹ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’å˜ä¸€ã«
        filename = re.sub(r'_+', '_', filename)
        
        # å‰å¾Œã®ç©ºç™½ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’é™¤å»
        filename = filename.strip(' _')
        
        # ç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåã‚’ä½¿ç”¨
        if not filename:
            filename = 'untitled'
        
        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ï¼ˆæ‹¡å¼µå­ã‚’è€ƒæ…®ã—ã¦200æ–‡å­—ä»¥å†…ï¼‰
        if len(filename) > 200:
            filename = filename[:200]
        
        return filename
    
    def add_excluded_domain(self, domain: str) -> None:
        """é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’è¿½åŠ """
        self.excluded_domains.add(domain.lower())
    
    def add_excluded_url(self, url: str) -> None:
        """é™¤å¤–URLã‚’è¿½åŠ """
        self.excluded_urls.add(url)
    
    def get_statistics(self, bookmarks: List[Bookmark]) -> Dict[str, int]:
        """
        ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        
        Args:
            bookmarks: ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä¸€è¦§
            
        Returns:
            Dict[str, int]: çµ±è¨ˆæƒ…å ±
        """
        total_bookmarks = len(bookmarks)
        unique_domains = len(set(urlparse(b.url).netloc for b in bookmarks))
        folder_count = len(set('/'.join(b.folder_path) for b in bookmarks if b.folder_path))
        
        return {
            'total_bookmarks': total_bookmarks,
            'unique_domains': unique_domains,
            'folder_count': folder_count
        }


class WebScraper:
    """
    Webãƒšãƒ¼ã‚¸å–å¾—ãƒ»è§£æã‚¯ãƒ©ã‚¹
    robots.txtç¢ºèªã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€è¨˜äº‹æœ¬æ–‡æŠ½å‡ºæ©Ÿèƒ½ã‚’æä¾›
    """
    
    def __init__(self):
        """WebScraperã‚’åˆæœŸåŒ–"""
        self.domain_last_access = {}  # ãƒ‰ãƒ¡ã‚¤ãƒ³ã”ã¨ã®æœ€çµ‚ã‚¢ã‚¯ã‚»ã‚¹æ™‚åˆ»
        self.rate_limit_delay = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å¾…ã¡æ™‚é–“ï¼ˆç§’ï¼‰
        self.timeout = 10  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
        self.user_agent = "Mozilla/5.0 (compatible; BookmarkToObsidian/1.0; +https://github.com/user/bookmark-to-obsidian)"
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        logger.info(f"ğŸŒ WebScraperåˆæœŸåŒ–å®Œäº† (User-Agent: {self.user_agent})")
    
    def check_robots_txt(self, domain: str) -> bool:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã®robots.txtã‚’ç¢ºèªã—ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒè¨±å¯ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            domain: ç¢ºèªå¯¾è±¡ã®ãƒ‰ãƒ¡ã‚¤ãƒ³
            
        Returns:
            bool: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒè¨±å¯ã•ã‚Œã¦ã„ã‚‹å ´åˆTrue
        """
        try:
            robots_url = f"https://{domain}/robots.txt"
            logger.debug(f"ğŸ¤– robots.txtç¢ºèª: {robots_url}")
            
            # RobotFileParserã‚’ä½¿ç”¨ã—ã¦robots.txtã‚’è§£æ
            rp = RobotFileParser()
            rp.set_url(robots_url)
            
            # robots.txtã‚’èª­ã¿è¾¼ã¿ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            try:
                rp.read()
                
                # User-Agentã«å¯¾ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã‚’ãƒã‚§ãƒƒã‚¯
                # ä¸€èˆ¬çš„ãªã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼åã¨ã‚«ã‚¹ã‚¿ãƒ User-Agentã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
                user_agents_to_check = [
                    self.user_agent,
                    "*",  # å…¨ã¦ã®User-Agent
                    "Mozilla/5.0",  # ä¸€èˆ¬çš„ãªãƒ–ãƒ©ã‚¦ã‚¶
                ]
                
                for ua in user_agents_to_check:
                    if rp.can_fetch(ua, "/"):
                        logger.debug(f"âœ… robots.txtè¨±å¯: {domain} (User-Agent: {ua})")
                        return True
                
                logger.info(f"ğŸš« robots.txtæ‹’å¦: {domain}")
                return False
                
            except Exception as e:
                # robots.txtãŒå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„å ´åˆã¯è¨±å¯ã¨ã¿ãªã™
                logger.debug(f"âš ï¸ robots.txtèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼ˆè¨±å¯ã¨ã—ã¦å‡¦ç†ï¼‰: {domain} - {str(e)}")
                return True
                
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å®‰å…¨å´ã«å€’ã—ã¦è¨±å¯ã¨ã¿ãªã™
            logger.debug(f"âš ï¸ robots.txtãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ï¼ˆè¨±å¯ã¨ã—ã¦å‡¦ç†ï¼‰: {domain} - {str(e)}")
            return True
    
    def fetch_page_content(self, url: str) -> Optional[str]:
        """
        Task 10: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å¼·åŒ–ã—ãŸãƒšãƒ¼ã‚¸å–å¾—æ©Ÿèƒ½
        
        Args:
            url: å–å¾—å¯¾è±¡ã®URL
            
        Returns:
            Optional[str]: å–å¾—ã•ã‚ŒãŸHTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
            
        Raises:
            requests.exceptions.ConnectionError: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼
            requests.exceptions.Timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼
            requests.exceptions.HTTPError: HTTPã‚¨ãƒ©ãƒ¼
            requests.exceptions.SSLError: SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼
        """
        try:
            # URLã®è§£æ
            parsed_url = urlparse(url)
            domain = parsed_url.netloc.lower()
            
            logger.debug(f"ğŸŒ ãƒšãƒ¼ã‚¸å–å¾—é–‹å§‹: {url}")
            
            # robots.txtãƒã‚§ãƒƒã‚¯
            if not self.check_robots_txt(domain):
                logger.info(f"ğŸš« robots.txtæ‹’å¦ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—: {url}")
                return None
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®é©ç”¨
            self.apply_rate_limiting(domain)
            
            # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
            try:
                response = self.session.get(
                    url,
                    timeout=self.timeout,
                    allow_redirects=True,
                    verify=True  # SSLè¨¼æ˜æ›¸æ¤œè¨¼ã‚’æœ‰åŠ¹åŒ–
                )
                
                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã®ç¢ºèª
                response.raise_for_status()
                
            except requests.exceptions.Timeout as e:
                logger.warning(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url} (timeout={self.timeout}s)")
                raise requests.exceptions.Timeout(f"ãƒšãƒ¼ã‚¸å–å¾—ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ: {url}")
                
            except requests.exceptions.SSLError as e:
                logger.warning(f"ğŸ”’ SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼: {url}")
                raise requests.exceptions.SSLError(f"SSLè¨¼æ˜æ›¸ã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {url}")
                
            except requests.exceptions.ConnectionError as e:
                logger.warning(f"ğŸ”Œ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {url}")
                raise requests.exceptions.ConnectionError(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {url}")
                
            except requests.exceptions.HTTPError as e:
                status_code = e.response.status_code if e.response else "ä¸æ˜"
                logger.warning(f"ğŸš« HTTPã‚¨ãƒ©ãƒ¼: {url} - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {status_code}")
                
                # ç‰¹å®šã®HTTPã‚¨ãƒ©ãƒ¼ã«å¯¾ã™ã‚‹è©³ç´°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                if status_code == 403:
                    raise requests.exceptions.HTTPError(f"ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸ (403): {url}")
                elif status_code == 404:
                    raise requests.exceptions.HTTPError(f"ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (404): {url}")
                elif status_code == 429:
                    raise requests.exceptions.HTTPError(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸ (429): {url}")
                elif status_code >= 500:
                    raise requests.exceptions.HTTPError(f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ ({status_code}): {url}")
                else:
                    raise requests.exceptions.HTTPError(f"HTTPã‚¨ãƒ©ãƒ¼ ({status_code}): {url}")
            
            # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®è‡ªå‹•æ¤œå‡º
            if response.encoding is None:
                response.encoding = response.apparent_encoding
            
            # HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            html_content = response.text
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚µã‚¤ã‚ºã®æ¤œè¨¼
            if len(html_content) < 100:
                logger.warning(f"âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¾ã™: {url} (ã‚µã‚¤ã‚º: {len(html_content)} æ–‡å­—)")
                return None
            
            logger.debug(f"âœ… ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ: {url} (ã‚µã‚¤ã‚º: {len(html_content):,} æ–‡å­—)")
            
            # æœ€çµ‚ã‚¢ã‚¯ã‚»ã‚¹æ™‚åˆ»ã‚’æ›´æ–°
            self.domain_last_access[domain] = time.time()
            
            return html_content
            
        except (requests.exceptions.Timeout, 
                requests.exceptions.ConnectionError, 
                requests.exceptions.HTTPError, 
                requests.exceptions.SSLError):
            # æ—¢çŸ¥ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã¯å†ç™ºç”Ÿã•ã›ã‚‹
            raise
            
        except Exception as e:
            logger.error(f"âŒ äºˆæœŸã—ãªã„ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {url} - {str(e)}")
            raise Exception(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    def extract_article_content(self, html: str, url: str = "") -> Optional[Dict]:
        """
        HTMLã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆé«˜åº¦ãªæŠ½å‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
        
        Args:
            html: HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            url: å…ƒã®URLï¼ˆãƒ­ã‚°ç”¨ï¼‰
            
        Returns:
            Optional[Dict]: æŠ½å‡ºã•ã‚ŒãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
            article_data = {
                'title': '',
                'content': '',
                'tags': [],
                'metadata': {},
                'quality_score': 0.0,
                'extraction_method': ''
            }
            
            # ä¸è¦ãªè¦ç´ ã‚’äº‹å‰ã«é™¤å»
            self._remove_unwanted_elements(soup)
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
            article_data['title'] = self._extract_title(soup, url)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            article_data['metadata'] = self._extract_metadata(soup)
            
            # ã‚¿ã‚°æƒ…å ±ã®æŠ½å‡º
            article_data['tags'] = self._extract_tags(soup, article_data['metadata'])
            
            # è¨˜äº‹æœ¬æ–‡ã®æŠ½å‡ºï¼ˆè¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦è¡Œï¼‰
            content_result = self._extract_main_content(soup, url)
            
            if content_result:
                article_data['content'] = content_result['content']
                article_data['quality_score'] = content_result['quality_score']
                article_data['extraction_method'] = content_result['method']
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªã®æ¤œè¨¼
                if self._validate_content_quality(article_data, url):
                    logger.debug(f"âœ… è¨˜äº‹æœ¬æ–‡æŠ½å‡ºæˆåŠŸ: {url} (æ–‡å­—æ•°: {len(article_data['content'])}, å“è³ªã‚¹ã‚³ã‚¢: {article_data['quality_score']:.2f}, æ–¹æ³•: {article_data['extraction_method']})")
                    return article_data
                else:
                    logger.warning(f"âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªãŒåŸºæº–ã‚’æº€ãŸã—ã¾ã›ã‚“: {url}")
                    return None
            else:
                logger.warning(f"âš ï¸ è¨˜äº‹æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {url}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ è¨˜äº‹æœ¬æ–‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {url} - {str(e)}")
            return None
    
    def _remove_unwanted_elements(self, soup: BeautifulSoup) -> None:
        """
        ä¸è¦ãªè¦ç´ ã‚’é™¤å»ï¼ˆåºƒå‘Šã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆãªã©ï¼‰
        
        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        # é™¤å»å¯¾è±¡ã®ã‚»ãƒ¬ã‚¯ã‚¿
        unwanted_selectors = [
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«
            'script', 'style', 'noscript',
            
            # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ 
            'nav', 'header', 'footer', 'aside',
            '.navigation', '.navbar', '.nav-menu', '.menu',
            '.breadcrumb', '.breadcrumbs',
            
            # åºƒå‘Šé–¢é€£
            '.advertisement', '.ads', '.ad', '.advert',
            '.google-ads', '.adsense', '.ad-container',
            '[id*="ad"]', '[class*="ad-"]', '[class*="ads-"]',
            
            # ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ»å…±æœ‰ãƒœã‚¿ãƒ³
            '.share-buttons', '.social-share', '.social-buttons',
            '.share', '.sharing', '.social-media',
            
            # ã‚³ãƒ¡ãƒ³ãƒˆãƒ»é–¢é€£è¨˜äº‹
            '.comments', '.comment-section', '.disqus',
            '.related-posts', '.related-articles', '.recommendations',
            
            # ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ»ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
            '.sidebar', '.widget', '.widgets',
            
            # ãã®ä»–ã®ä¸è¦è¦ç´ 
            '.popup', '.modal', '.overlay',
            '.newsletter', '.subscription',
            '.cookie-notice', '.cookie-banner',
            '.back-to-top', '.scroll-to-top'
        ]
        
        for selector in unwanted_selectors:
            for element in soup.select(selector):
                element.decompose()
    
    def _extract_title(self, soup: BeautifulSoup, url: str) -> str:
        """
        ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
        
        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            url: å…ƒã®URL
            
        Returns:
            str: æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«
        """
        # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã®å„ªå…ˆé †ä½
        title_selectors = [
            'h1',  # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
            'title',  # HTMLã‚¿ã‚¤ãƒˆãƒ«
            '[property="og:title"]',  # Open Graphã‚¿ã‚¤ãƒˆãƒ«
            '.title', '.post-title', '.article-title',
            '.entry-title', '.page-title'
        ]
        
        for selector in title_selectors:
            elements = soup.select(selector)
            for element in elements:
                if selector == '[property="og:title"]':
                    title = element.get('content', '').strip()
                else:
                    title = element.get_text(strip=True)
                
                if title and len(title) > 5:  # æœ€å°æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
                    # ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    title = re.sub(r'\s+', ' ', title)  # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’å˜ä¸€ã«
                    title = title.replace('\n', ' ').replace('\t', ' ')
                    return title[:200]  # æœ€å¤§200æ–‡å­—ã«åˆ¶é™
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯URLã‹ã‚‰ç”Ÿæˆ
        from urllib.parse import urlparse
        parsed_url = urlparse(url)
        return f"è¨˜äº‹ - {parsed_url.netloc}"
    
    def _extract_metadata(self, soup: BeautifulSoup) -> Dict[str, str]:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        
        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            
        Returns:
            Dict[str, str]: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        metadata = {}
        
        # ãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            name = meta.get('name', '').lower()
            property_attr = meta.get('property', '').lower()
            content = meta.get('content', '').strip()
            
            if content:
                # æ¨™æº–çš„ãªãƒ¡ã‚¿ã‚¿ã‚°
                if name in ['description', 'keywords', 'author', 'robots', 'viewport']:
                    metadata[name] = content
                
                # Open Graphã‚¿ã‚°
                elif property_attr.startswith('og:'):
                    metadata[property_attr] = content
                
                # Articleã‚¿ã‚°
                elif property_attr.startswith('article:'):
                    metadata[property_attr] = content
                
                # Twitterã‚«ãƒ¼ãƒ‰
                elif name.startswith('twitter:'):
                    metadata[name] = content
        
        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆJSON-LDï¼‰ã®æŠ½å‡º
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_ld_scripts:
            try:
                import json
                data = json.loads(script.string)
                if isinstance(data, dict):
                    if data.get('@type') in ['Article', 'BlogPosting', 'NewsArticle']:
                        if 'author' in data:
                            metadata['structured_author'] = str(data['author'])
                        if 'datePublished' in data:
                            metadata['structured_date'] = data['datePublished']
                        if 'description' in data:
                            metadata['structured_description'] = data['description']
            except (json.JSONDecodeError, AttributeError):
                continue
        
        return metadata
    
    def _extract_tags(self, soup: BeautifulSoup, metadata: Dict[str, str]) -> List[str]:
        """
        ã‚¿ã‚°æƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            List[str]: æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚°ä¸€è¦§
        """
        tags = set()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æŠ½å‡º
        keywords = metadata.get('keywords', '')
        if keywords:
            # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†å‰²
            keyword_list = [kw.strip() for kw in keywords.split(',') if kw.strip()]
            tags.update(keyword_list)
        
        # HTMLã‹ã‚‰ã‚¿ã‚°è¦ç´ ã‚’æŠ½å‡º
        tag_selectors = [
            '.tags a', '.tag a', '.categories a', '.category a',
            '.labels a', '.label a', '.topics a', '.topic a',
            '[rel="tag"]', '.post-tags a', '.entry-tags a'
        ]
        
        for selector in tag_selectors:
            elements = soup.select(selector)
            for element in elements:
                tag_text = element.get_text(strip=True)
                if tag_text and len(tag_text) <= 50:  # æœ€å¤§50æ–‡å­—ã®ã‚¿ã‚°ã®ã¿
                    # ã‚¿ã‚°ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                    tag_text = re.sub(r'[^\w\s\-_]', '', tag_text)  # ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»
                    tag_text = re.sub(r'\s+', '-', tag_text.strip())  # ã‚¹ãƒšãƒ¼ã‚¹ã‚’ãƒã‚¤ãƒ•ãƒ³ã«
                    if tag_text:
                        tags.add(tag_text)
        
        # ã‚¿ã‚°æ•°ã‚’åˆ¶é™ï¼ˆæœ€å¤§20å€‹ï¼‰
        return list(tags)[:20]
    
    def _extract_main_content(self, soup: BeautifulSoup, url: str) -> Optional[Dict]:
        """
        ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦è¡Œï¼‰
        
        Args:
            soup: BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            url: å…ƒã®URL
            
        Returns:
            Optional[Dict]: æŠ½å‡ºçµæœï¼ˆcontent, quality_score, methodï¼‰
        """
        extraction_methods = [
            ('semantic_tags', self._extract_by_semantic_tags),
            ('content_density', self._extract_by_content_density),
            ('common_selectors', self._extract_by_common_selectors),
            ('body_fallback', self._extract_by_body_fallback)
        ]
        
        best_result = None
        best_score = 0.0
        
        for method_name, method_func in extraction_methods:
            try:
                result = method_func(soup)
                if result and result['quality_score'] > best_score:
                    best_result = result
                    best_score = result['quality_score']
                    best_result['method'] = method_name
                    
                    # ååˆ†ã«é«˜å“è³ªãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯æ—©æœŸçµ‚äº†
                    if best_score >= 0.8:
                        break
                        
            except Exception as e:
                logger.debug(f"æŠ½å‡ºæ–¹æ³• {method_name} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        return best_result
    
    def _extract_by_semantic_tags(self, soup: BeautifulSoup) -> Optional[Dict]:
        """
        ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¿ã‚°ã‚’ä½¿ç”¨ã—ãŸæŠ½å‡º
        """
        semantic_selectors = ['article', 'main', '[role="main"]']
        
        for selector in semantic_selectors:
            elements = soup.select(selector)
            if elements:
                # æœ€ã‚‚é•·ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é¸æŠ
                best_element = max(elements, key=lambda x: len(x.get_text()))
                content = self._clean_content(best_element.get_text())
                
                if len(content) > 50:  # é–¾å€¤ã‚’ä¸‹ã’ã‚‹
                    return {
                        'content': content,
                        'quality_score': 0.9,  # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¿ã‚°ã¯é«˜å“è³ª
                        'method': 'semantic_tags'
                    }
        
        return None
    
    def _extract_by_content_density(self, soup: BeautifulSoup) -> Optional[Dict]:
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯†åº¦ã«ã‚ˆã‚‹æŠ½å‡º
        """
        # å„è¦ç´ ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¯†åº¦ã‚’è¨ˆç®—
        candidates = []
        
        for element in soup.find_all(['div', 'section', 'article']):
            text = element.get_text(strip=True)
            if len(text) < 50:  # é–¾å€¤ã‚’ä¸‹ã’ã‚‹
                continue
            
            # ãƒªãƒ³ã‚¯å¯†åº¦ã‚’è¨ˆç®—ï¼ˆãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆ / å…¨ãƒ†ã‚­ã‚¹ãƒˆï¼‰
            link_text = ''.join([a.get_text() for a in element.find_all('a')])
            link_density = len(link_text) / len(text) if text else 1.0
            
            # æ®µè½æ•°ã‚’è¨ˆç®—
            paragraphs = len(element.find_all('p'))
            
            # å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            quality_score = (
                min(len(text) / 500, 1.0) * 0.4 +  # æ–‡å­—æ•°ï¼ˆæœ€å¤§500æ–‡å­—ã§1.0ï¼‰
                (1.0 - link_density) * 0.4 +  # ãƒªãƒ³ã‚¯å¯†åº¦ãŒä½ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
                min(paragraphs / 3, 1.0) * 0.2  # æ®µè½æ•°ï¼ˆæœ€å¤§3æ®µè½ã§1.0ï¼‰
            )
            
            candidates.append({
                'element': element,
                'text': text,
                'quality_score': quality_score
            })
        
        if candidates:
            # æœ€é«˜ã‚¹ã‚³ã‚¢ã®è¦ç´ ã‚’é¸æŠ
            best_candidate = max(candidates, key=lambda x: x['quality_score'])
            content = self._clean_content(best_candidate['text'])
            
            return {
                'content': content,
                'quality_score': best_candidate['quality_score'],
                'method': 'content_density'
            }
        
        return None
    
    def _extract_by_common_selectors(self, soup: BeautifulSoup) -> Optional[Dict]:
        """
        ä¸€èˆ¬çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ã«ã‚ˆã‚‹æŠ½å‡º
        """
        common_selectors = [
            '.content', '.post-content', '.entry-content',
            '.article-content', '#content', '#main-content',
            '.main-content', '.post-body', '.entry-body',
            '.article-body', '.content-body'
        ]
        
        for selector in common_selectors:
            elements = soup.select(selector)
            if elements:
                best_element = max(elements, key=lambda x: len(x.get_text()))
                content = self._clean_content(best_element.get_text())
                
                if len(content) > 100:
                    return {
                        'content': content,
                        'quality_score': 0.7,  # ä¸­ç¨‹åº¦ã®å“è³ª
                        'method': 'common_selectors'
                    }
        
        return None
    
    def _extract_by_body_fallback(self, soup: BeautifulSoup) -> Optional[Dict]:
        """
        bodyã‚¿ã‚°ã‹ã‚‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æŠ½å‡º
        """
        body = soup.find('body')
        if body:
            content = self._clean_content(body.get_text())
            
            if len(content) > 200:
                return {
                    'content': content,
                    'quality_score': 0.3,  # ä½å“è³ªï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    'method': 'body_fallback'
                }
        
        return None
    
    def _clean_content(self, text: str) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        
        Args:
            text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            str: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not text:
            return ""
        
        # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’å˜ä¸€ã®ã‚¹ãƒšãƒ¼ã‚¹ã«
        text = re.sub(r'[ \t]+', ' ', text)
        
        # è¡Œã”ã¨ã«å‡¦ç†
        lines = []
        for line in text.split('\n'):
            line = line.strip()
            if line and len(line) > 3:  # çŸ­ã™ãã‚‹è¡Œã¯é™¤å¤–
                lines.append(line)
        
        # æ®µè½ã¨ã—ã¦çµåˆ
        content = '\n\n'.join(lines)
        
        # æœ€å¤§æ–‡å­—æ•°åˆ¶é™ï¼ˆ10,000æ–‡å­—ï¼‰
        if len(content) > 10000:
            content = content[:10000] + "..."
        
        return content
    
    def _validate_content_quality(self, article_data: Dict, url: str) -> bool:
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªã‚’æ¤œè¨¼
        
        Args:
            article_data: è¨˜äº‹ãƒ‡ãƒ¼ã‚¿
            url: å…ƒã®URL
            
        Returns:
            bool: å“è³ªåŸºæº–ã‚’æº€ãŸã™å ´åˆTrue
        """
        content = article_data.get('content', '')
        quality_score = article_data.get('quality_score', 0.0)
        
        # åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
        checks = {
            'min_length': len(content) >= 100,  # æœ€å°100æ–‡å­—
            'max_length': len(content) <= 50000,  # æœ€å¤§50,000æ–‡å­—
            'quality_score': quality_score >= 0.3,  # æœ€å°å“è³ªã‚¹ã‚³ã‚¢
            'has_title': bool(article_data.get('title', '').strip()),  # ã‚¿ã‚¤ãƒˆãƒ«å­˜åœ¨
            'reasonable_structure': content.count('\n') >= 2  # æœ€ä½é™ã®æ§‹é€ 
        }
        
        # ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ã‚’ãƒ‘ã‚¹
        passed_checks = sum(checks.values())
        total_checks = len(checks)
        
        success_rate = passed_checks / total_checks
        
        if success_rate < 0.8:  # 80%ä»¥ä¸Šã®ãƒã‚§ãƒƒã‚¯ã‚’ãƒ‘ã‚¹ã™ã‚‹å¿…è¦
            logger.debug(f"å“è³ªãƒã‚§ãƒƒã‚¯å¤±æ•—: {url} - {checks}")
            return False
        
        # ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãªã©ï¼‰
        error_patterns = [
            r'404.*not found',
            r'page not found',
            r'access denied',
            r'forbidden',
            r'error occurred'
        ]
        
        content_lower = content.lower()
        for pattern in error_patterns:
            if re.search(pattern, content_lower):
                logger.debug(f"ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º: {url} - {pattern}")
                return False
        
        return True
    
    def group_urls_by_domain(self, urls: List[str]) -> Dict[str, List[str]]:
        """
        URLãƒªã‚¹ãƒˆã‚’ãƒ‰ãƒ¡ã‚¤ãƒ³ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        
        Args:
            urls: URLä¸€è¦§
            
        Returns:
            Dict[str, List[str]]: ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ã‚­ãƒ¼ã¨ã—ãŸURLä¸€è¦§
        """
        domain_groups = {}
        
        for url in urls:
            try:
                parsed_url = urlparse(url)
                domain = parsed_url.netloc.lower()
                
                if domain not in domain_groups:
                    domain_groups[domain] = []
                
                domain_groups[domain].append(url)
                
            except Exception as e:
                logger.warning(f"âš ï¸ URLè§£æã‚¨ãƒ©ãƒ¼: {url} - {str(e)}")
                continue
        
        logger.info(f"ğŸŒ ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—åŒ–å®Œäº†: {len(domain_groups)}å€‹ã®ãƒ‰ãƒ¡ã‚¤ãƒ³")
        for domain, domain_urls in domain_groups.items():
            logger.debug(f"  ğŸ“ {domain}: {len(domain_urls)}å€‹ã®URL")
        
        return domain_groups
    
    def apply_rate_limiting(self, domain: str) -> None:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã«å¯¾ã—ã¦ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é©ç”¨
        
        Args:
            domain: å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³
        """
        current_time = time.time()
        
        if domain in self.domain_last_access:
            time_since_last_access = current_time - self.domain_last_access[domain]
            
            if time_since_last_access < self.rate_limit_delay:
                sleep_time = self.rate_limit_delay - time_since_last_access
                logger.debug(f"â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¾…æ©Ÿ: {domain} ({sleep_time:.1f}ç§’)")
                time.sleep(sleep_time)
        
        # æœ€çµ‚ã‚¢ã‚¯ã‚»ã‚¹æ™‚åˆ»ã‚’æ›´æ–°
        self.domain_last_access[domain] = time.time()
    
    def set_rate_limit_delay(self, delay: float) -> None:
        """
        ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å¾…ã¡æ™‚é–“ã‚’è¨­å®š
        
        Args:
            delay: å¾…ã¡æ™‚é–“ï¼ˆç§’ï¼‰
        """
        self.rate_limit_delay = max(1.0, delay)  # æœ€å°1ç§’
        logger.info(f"â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®š: {self.rate_limit_delay}ç§’")
    
    def set_timeout(self, timeout: int) -> None:
        """
        ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
        
        Args:
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰
        """
        self.timeout = max(5, timeout)  # æœ€å°5ç§’
        logger.info(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: {self.timeout}ç§’")
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        WebScraperçµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        
        Returns:
            Dict[str, Any]: çµ±è¨ˆæƒ…å ±
        """
        return {
            'domains_accessed': len(self.domain_last_access),
            'rate_limit_delay': self.rate_limit_delay,
            'timeout': self.timeout,
            'user_agent': self.user_agent
        }


class MarkdownGenerator:
    """
    Obsidianå½¢å¼ã®Markdownç”Ÿæˆã‚¯ãƒ©ã‚¹
    è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Obsidianç”¨ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
    """
    
    def __init__(self):
        """MarkdownGeneratorã‚’åˆæœŸåŒ–"""
        self.yaml_template = {
            'title': '',
            'url': '',
            'created': '',
            'tags': [],
            'description': '',
            'author': '',
            'source': 'bookmark-to-obsidian'
        }
        
        logger.info("ğŸ“ MarkdownGeneratoråˆæœŸåŒ–å®Œäº†")
    
    def generate_obsidian_markdown(self, page_data: Dict, bookmark: 'Bookmark') -> str:
        """
        ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Obsidianå½¢å¼ã®Markdownã‚’ç”Ÿæˆ
        
        Args:
            page_data: WebScraperã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿
            bookmark: ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æƒ…å ±
            
        Returns:
            str: Obsidianå½¢å¼ã®Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        """
        try:
            # YAML front matterã‚’ç”Ÿæˆ
            yaml_frontmatter = self._create_yaml_frontmatter(page_data, bookmark)
            
            # è¨˜äº‹æœ¬æ–‡ã‚’Markdownå½¢å¼ã«å¤‰æ›
            markdown_content = self._format_content_for_obsidian(page_data.get('content', ''))
            
            # ã‚¿ã‚°ã‚’Obsidianå½¢å¼ã«å¤‰æ›
            obsidian_tags = self._format_tags_for_obsidian(page_data.get('tags', []))
            
            # å®Œå…¨ãªMarkdownã‚’æ§‹ç¯‰
            full_markdown = self._build_complete_markdown(
                yaml_frontmatter, 
                markdown_content, 
                obsidian_tags,
                page_data,
                bookmark
            )
            
            logger.debug(f"ğŸ“ Markdownç”ŸæˆæˆåŠŸ: {bookmark.title} (æ–‡å­—æ•°: {len(full_markdown)})")
            return full_markdown
            
        except Exception as e:
            logger.error(f"âŒ Markdownç”Ÿæˆã‚¨ãƒ©ãƒ¼: {bookmark.title} - {str(e)}")
            return self._generate_fallback_markdown(bookmark)
    
    def _create_yaml_frontmatter(self, page_data: Dict, bookmark: 'Bookmark') -> str:
        """
        YAML front matterã‚’ç”Ÿæˆ
        
        Args:
            page_data: è¨˜äº‹ãƒ‡ãƒ¼ã‚¿
            bookmark: ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æƒ…å ±
            
        Returns:
            str: YAML front matteræ–‡å­—åˆ—
        """
        import datetime
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        yaml_data = self.yaml_template.copy()
        
        # åŸºæœ¬æƒ…å ±
        yaml_data['title'] = page_data.get('title', bookmark.title)
        yaml_data['url'] = bookmark.url
        yaml_data['created'] = datetime.datetime.now().isoformat()
        
        # ã‚¿ã‚°æƒ…å ±
        tags = page_data.get('tags', [])
        if tags:
            yaml_data['tags'] = tags
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’æŠ½å‡º
        metadata = page_data.get('metadata', {})
        if metadata.get('description'):
            yaml_data['description'] = metadata['description']
        if metadata.get('author'):
            yaml_data['author'] = metadata['author']
        
        # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æƒ…å ±
        if bookmark.add_date:
            yaml_data['bookmarked'] = bookmark.add_date.isoformat()
        
        if bookmark.folder_path:
            yaml_data['folder'] = '/'.join(bookmark.folder_path)
        
        # å“è³ªæƒ…å ±
        if 'quality_score' in page_data:
            yaml_data['quality_score'] = page_data['quality_score']
        if 'extraction_method' in page_data:
            yaml_data['extraction_method'] = page_data['extraction_method']
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªYAMLç”Ÿæˆï¼ˆyamlãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã‚ãªã„ï¼‰
        return self._create_simple_yaml_frontmatter_dict(yaml_data)
    
    def _create_simple_yaml_frontmatter_dict(self, yaml_data: Dict) -> str:
        """
        è¾æ›¸ã‹ã‚‰ã‚·ãƒ³ãƒ—ãƒ«ãªYAML front matterã‚’ç”Ÿæˆ
        
        Args:
            yaml_data: YAMLç”¨ãƒ‡ãƒ¼ã‚¿è¾æ›¸
            
        Returns:
            str: YAML front matteræ–‡å­—åˆ—
        """
        lines = ["---"]
        
        for key, value in yaml_data.items():
            if value:  # ç©ºã§ãªã„å€¤ã®ã¿
                if isinstance(value, list):
                    if value:  # ç©ºã§ãªã„ãƒªã‚¹ãƒˆã®ã¿
                        lines.append(f"{key}:")
                        for item in value:
                            lines.append(f"  - \"{self._escape_yaml_string(str(item))}\"")
                elif isinstance(value, (int, float)):
                    lines.append(f"{key}: {value}")
                else:
                    lines.append(f"{key}: \"{self._escape_yaml_string(str(value))}\"")
        
        lines.append("---")
        return '\n'.join(lines) + '\n'
    
    def _escape_yaml_string(self, text: str) -> str:
        """
        YAMLæ–‡å­—åˆ—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
        
        Args:
            text: ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å¯¾è±¡ã®æ–‡å­—åˆ—
            
        Returns:
            str: ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸæ–‡å­—åˆ—
        """
        if not text:
            return ""
        
        # å±é™ºãªæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
        text = text.replace('"', '\\"')
        text = text.replace('\n', '\\n')
        text = text.replace('\r', '\\r')
        text = text.replace('\t', '\\t')
        
        return text
    
    def _format_content_for_obsidian(self, content: str) -> str:
        """
        è¨˜äº‹æœ¬æ–‡ã‚’Obsidianç”¨ã®Markdownå½¢å¼ã«å¤‰æ›
        
        Args:
            content: å…ƒã®è¨˜äº‹æœ¬æ–‡
            
        Returns:
            str: Obsidianå½¢å¼ã®Markdown
        """
        if not content:
            return ""
        
        # åŸºæœ¬çš„ãªMarkdownå¤‰æ›
        formatted_content = content
        
        # æ®µè½ã®æ•´ç†
        paragraphs = [p.strip() for p in formatted_content.split('\n\n') if p.strip()]
        
        # Obsidianç‰¹æœ‰ã®å‡¦ç†
        processed_paragraphs = []
        for paragraph in paragraphs:
            # é•·ã„æ®µè½ã‚’é©åˆ‡ã«åˆ†å‰²
            if len(paragraph) > 500:
                sentences = self._split_into_sentences(paragraph)
                processed_paragraphs.extend(sentences)
            else:
                processed_paragraphs.append(paragraph)
        
        # æœ€çµ‚çš„ãªMarkdownã‚’æ§‹ç¯‰
        markdown_content = '\n\n'.join(processed_paragraphs)
        
        # Obsidianç‰¹æœ‰ã®è¨˜æ³•ã‚’é©ç”¨
        markdown_content = self._apply_obsidian_formatting(markdown_content)
        
        return markdown_content
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """
        é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡å˜ä½ã§åˆ†å‰²
        
        Args:
            text: åˆ†å‰²å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            List[str]: åˆ†å‰²ã•ã‚ŒãŸæ–‡ã®ãƒªã‚¹ãƒˆ
        """
        import re
        
        # æ—¥æœ¬èªã¨è‹±èªã®æ–‡åŒºåˆ‡ã‚Šãƒ‘ã‚¿ãƒ¼ãƒ³
        sentence_patterns = [
            r'[ã€‚ï¼ï¼Ÿ]',  # æ—¥æœ¬èªã®æ–‡æœ«
            r'[.!?](?:\s|$)',  # è‹±èªã®æ–‡æœ«
        ]
        
        sentences = []
        current_sentence = ""
        
        for char in text:
            current_sentence += char
            
            # æ–‡æœ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            for pattern in sentence_patterns:
                if re.search(pattern, current_sentence[-2:]):
                    if len(current_sentence.strip()) > 10:  # æœ€å°æ–‡å­—æ•°
                        sentences.append(current_sentence.strip())
                        current_sentence = ""
                    break
        
        # æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
        if current_sentence.strip():
            sentences.append(current_sentence.strip())
        
        return sentences
    
    def _apply_obsidian_formatting(self, content: str) -> str:
        """
        Obsidianç‰¹æœ‰ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’é©ç”¨
        
        Args:
            content: å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            
        Returns:
            str: Obsidianå½¢å¼ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        """
        # URLã‚’è‡ªå‹•ãƒªãƒ³ã‚¯åŒ–ï¼ˆæ—¢å­˜ã®Markdownãƒªãƒ³ã‚¯ã¯é™¤å¤–ï¼‰
        import re
        
        # æ—¢å­˜ã®Markdownãƒªãƒ³ã‚¯ã‚’ä¿è­·ã™ã‚‹ãŸã‚ã€ã¾ãšæ—¢å­˜ã®ãƒªãƒ³ã‚¯ã‚’æ¤œå‡º
        existing_links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
        
        # æ—¢å­˜ã®ãƒªãƒ³ã‚¯ä»¥å¤–ã®HTTPSãƒªãƒ³ã‚¯ã‚’æ¤œå‡ºã—ã¦Obsidianå½¢å¼ã«å¤‰æ›
        # ãŸã ã—ã€æ—¢ã«Markdownãƒªãƒ³ã‚¯å†…ã«ã‚ã‚‹URLã¯å¤‰æ›ã—ãªã„
        def replace_url(match):
            url = match.group()
            # æ—¢å­˜ã®ãƒªãƒ³ã‚¯å†…ã®URLã‹ãƒã‚§ãƒƒã‚¯
            for link_text, link_url in existing_links:
                if url in link_url:
                    return url  # æ—¢å­˜ã®ãƒªãƒ³ã‚¯å†…ãªã®ã§å¤‰æ›ã—ãªã„
            return f"[{url}]({url})"
        
        # å˜ç‹¬ã®URLã®ã¿ã‚’å¤‰æ›ï¼ˆMarkdownãƒªãƒ³ã‚¯å†…ã§ãªã„ã‚‚ã®ï¼‰
        url_pattern = r'(?<!\]\()https?://[^\s<>"\']+[^\s<>"\'.,;:!?](?!\))'
        content = re.sub(url_pattern, replace_url, content)
        
        return content
    
    def _format_tags_for_obsidian(self, tags: List[str]) -> str:
        """
        ã‚¿ã‚°ã‚’Obsidianå½¢å¼ï¼ˆ#ã‚¿ã‚°åï¼‰ã«å¤‰æ›
        
        Args:
            tags: ã‚¿ã‚°ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            str: Obsidianå½¢å¼ã®ã‚¿ã‚°æ–‡å­—åˆ—
        """
        if not tags:
            return ""
        
        obsidian_tags = []
        
        for tag in tags:
            # ã‚¿ã‚°ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            clean_tag = self._clean_tag_for_obsidian(tag)
            if clean_tag:
                obsidian_tags.append(f"#{clean_tag}")
        
        if obsidian_tags:
            return "\n\n## ã‚¿ã‚°\n\n" + " ".join(obsidian_tags)
        
        return ""
    
    def _clean_tag_for_obsidian(self, tag: str) -> str:
        """
        ã‚¿ã‚°ã‚’Obsidianç”¨ã«ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        
        Args:
            tag: å…ƒã®ã‚¿ã‚°
            
        Returns:
            str: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸã‚¿ã‚°
        """
        import re
        
        if not tag:
            return ""
        
        clean_tag = tag.strip()
        
        # ã‚¹ãƒšãƒ¼ã‚¹ã‚’ãƒã‚¤ãƒ•ãƒ³ã«å¤‰æ›
        clean_tag = re.sub(r'\s+', '-', clean_tag)
        
        # ç‰¹æ®Šæ–‡å­—ã‚’ãƒã‚¤ãƒ•ãƒ³ã«å¤‰æ›ï¼ˆã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã€ãƒ‰ãƒƒãƒˆãªã©ï¼‰
        clean_tag = re.sub(r'[/\\.+]', '-', clean_tag)
        
        # è¨±å¯ã•ã‚Œãªã„æ–‡å­—ã‚’é™¤å»ï¼ˆè‹±æ•°å­—ã€æ—¥æœ¬èªã€ãƒã‚¤ãƒ•ãƒ³ã€ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿ï¼‰
        clean_tag = re.sub(r'[^\w\-ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¯]', '', clean_tag)
        
        # å…ˆé ­ã¨æœ«å°¾ã®ãƒã‚¤ãƒ•ãƒ³ã‚’é™¤å»
        clean_tag = clean_tag.strip('-_')
        
        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(clean_tag) > 50:
            clean_tag = clean_tag[:50]
        
        return clean_tag
    
    def _build_complete_markdown(self, yaml_frontmatter: str, content: str, tags: str, page_data: Dict, bookmark: 'Bookmark') -> str:
        """
        å®Œå…¨ãªMarkdownãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ§‹ç¯‰
        
        Args:
            yaml_frontmatter: YAML front matter
            content: è¨˜äº‹æœ¬æ–‡
            tags: Obsidianã‚¿ã‚°
            page_data: è¨˜äº‹ãƒ‡ãƒ¼ã‚¿
            bookmark: ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æƒ…å ±
            
        Returns:
            str: å®Œå…¨ãªMarkdownãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        sections = [yaml_frontmatter]
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        title = page_data.get('title', bookmark.title)
        sections.append(f"# {title}\n")
        
        # å…ƒURLæƒ…å ±
        sections.append(f"**å…ƒURL:** [{bookmark.url}]({bookmark.url})\n")
        
        # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ—¥æ™‚
        if bookmark.add_date:
            sections.append(f"**ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ—¥æ™‚:** {bookmark.add_date.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±
        if bookmark.folder_path:
            folder_path = ' > '.join(bookmark.folder_path)
            sections.append(f"**ãƒ•ã‚©ãƒ«ãƒ€:** {folder_path}\n")
        
        # è¨˜äº‹æœ¬æ–‡
        if content:
            sections.append("## è¨˜äº‹å†…å®¹\n")
            sections.append(content)
        
        # ã‚¿ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if tags:
            sections.append(tags)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        metadata = page_data.get('metadata', {})
        if metadata:
            sections.append("\n## ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\n")
            
            if metadata.get('description'):
                sections.append(f"**èª¬æ˜:** {metadata['description']}\n")
            
            if metadata.get('author'):
                sections.append(f"**è‘—è€…:** {metadata['author']}\n")
            
            # å“è³ªæƒ…å ±
            if 'quality_score' in page_data:
                sections.append(f"**å“è³ªã‚¹ã‚³ã‚¢:** {page_data['quality_score']:.2f}\n")
            
            if 'extraction_method' in page_data:
                sections.append(f"**æŠ½å‡ºæ–¹æ³•:** {page_data['extraction_method']}\n")
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’çµåˆ
        return '\n'.join(sections)
    
    def _generate_fallback_markdown(self, bookmark: 'Bookmark') -> str:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªMarkdownã‚’ç”Ÿæˆ
        
        Args:
            bookmark: ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æƒ…å ±
            
        Returns:
            str: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨Markdown
        """
        import datetime
        
        lines = [
            "---",
            f"title: \"{self._escape_yaml_string(bookmark.title)}\"",
            f"url: \"{bookmark.url}\"",
            f"created: \"{datetime.datetime.now().isoformat()}\"",
            "source: \"bookmark-to-obsidian\"",
            "status: \"extraction_failed\"",
            "---",
            "",
            f"# {bookmark.title}",
            "",
            f"**å…ƒURL:** [{bookmark.url}]({bookmark.url})",
            "",
            "## æ³¨æ„",
            "",
            "ã“ã®ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’è‡ªå‹•æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
            "å…ƒã®URLã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
            ""
        ]
        
        if bookmark.add_date:
            lines.insert(-3, f"**ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ—¥æ™‚:** {bookmark.add_date.strftime('%Y-%m-%d %H:%M:%S')}")
        
        if bookmark.folder_path:
            folder_path = ' > '.join(bookmark.folder_path)
            lines.insert(-3, f"**ãƒ•ã‚©ãƒ«ãƒ€:** {folder_path}")
        
        return '\n'.join(lines)
    
    def generate_file_path(self, bookmark: 'Bookmark', base_path: Path) -> Path:
        """
        ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯éšå±¤æ§‹é€ ã‚’ç¶­æŒã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
        
        Args:
            bookmark: ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æƒ…å ±
            base_path: åŸºæº–ãƒ‘ã‚¹
            
        Returns:
            Path: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        try:
            # ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ã‚’æ§‹ç¯‰
            folder_parts = []
            if bookmark.folder_path:
                for folder in bookmark.folder_path:
                    # ãƒ•ã‚©ãƒ«ãƒ€åã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ç”¨ã«ã‚µãƒ‹ã‚¿ã‚¤ã‚º
                    clean_folder = self._sanitize_path_component(folder)
                    if clean_folder:
                        folder_parts.append(clean_folder)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            filename = self._sanitize_path_component(bookmark.title)
            if not filename:
                filename = "untitled"
            
            # æ‹¡å¼µå­ã‚’è¿½åŠ 
            filename += ".md"
            
            # å®Œå…¨ãªãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            if folder_parts:
                full_path = base_path / Path(*folder_parts) / filename
            else:
                full_path = base_path / filename
            
            logger.debug(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ç”Ÿæˆ: {full_path}")
            return full_path
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {bookmark.title} - {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
            safe_filename = f"bookmark_{hash(bookmark.url) % 10000}.md"
            return base_path / safe_filename
    
    def _sanitize_path_component(self, name: str) -> str:
        """
        ãƒ‘ã‚¹è¦ç´ ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ç”¨ã«ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        
        Args:
            name: å…ƒã®åå‰
            
        Returns:
            str: ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã•ã‚ŒãŸåå‰
        """
        import re
        
        if not name:
            return ""
        
        # å±é™ºãªæ–‡å­—ã‚’é™¤å»ãƒ»ç½®æ›
        sanitized = re.sub(r'[<>:"/\\|?*]', '_', name)
        
        # é€£ç¶šã™ã‚‹ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’å˜ä¸€ã«
        sanitized = re.sub(r'_+', '_', sanitized)
        
        # å‰å¾Œã®ç©ºç™½ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’é™¤å»
        sanitized = sanitized.strip(' _.')
        
        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(sanitized) > 100:
            sanitized = sanitized[:100]
        
        # äºˆç´„èªã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆWindowsï¼‰
        reserved_names = ['CON', 'PRN', 'AUX', 'NUL', 'COM1', 'COM2', 'COM3', 'COM4', 'COM5', 'COM6', 'COM7', 'COM8', 'COM9', 'LPT1', 'LPT2', 'LPT3', 'LPT4', 'LPT5', 'LPT6', 'LPT7', 'LPT8', 'LPT9']
        if sanitized.upper() in reserved_names:
            sanitized = f"_{sanitized}"
        
        return sanitized
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        MarkdownGeneratorçµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        
        Returns:
            Dict[str, Any]: çµ±è¨ˆæƒ…å ±
        """
        return {
            'yaml_template_keys': len(self.yaml_template),
            'supported_formats': ['obsidian', 'yaml', 'markdown']
        }


def validate_bookmarks_file(uploaded_file) -> tuple[bool, str]:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒbookmarks.htmlã¨ã—ã¦æœ‰åŠ¹ã‹ã‚’æ¤œè¨¼ã™ã‚‹
    
    Returns:
        tuple[bool, str]: (æ¤œè¨¼çµæœ, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¾ãŸã¯æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
    """
    if uploaded_file is None:
        return False, "ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“"
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ç¢ºèª
    if not uploaded_file.name.lower().endswith('.html'):
        return False, "HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿å–ã‚Š
        content = uploaded_file.read().decode('utf-8')
        uploaded_file.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
        
        # BeautifulSoupã§HTMLã‚’è§£æ
        soup = BeautifulSoup(content, 'html.parser')
        
        # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ§‹é€ ã®åŸºæœ¬çš„ãªç¢ºèª
        # Chromeã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯é€šå¸¸<DT><A>ã‚¿ã‚°ãŒå«ã¾ã‚Œã‚‹
        bookmark_links = soup.find_all('a')
        if len(bookmark_links) == 0:
            return False, "ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ­£ã—ã„bookmarks.htmlãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
        
        # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã®ç¢ºèªï¼ˆ<DT><H3>ã‚¿ã‚°ï¼‰
        folder_headers = soup.find_all('h3')
        
        return True, f"âœ… æœ‰åŠ¹ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ï¼ˆ{len(bookmark_links)}å€‹ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€{len(folder_headers)}å€‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œå‡ºï¼‰"
        
    except UnicodeDecodeError:
        return False, "ãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"
    except Exception as e:
        return False, f"ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {str(e)}"


def validate_directory_path(directory_path: str) -> tuple[bool, str]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ãŒæœ‰åŠ¹ã‹ã‚’æ¤œè¨¼ã™ã‚‹
    
    Returns:
        tuple[bool, str]: (æ¤œè¨¼çµæœ, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¾ãŸã¯æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
    """
    if not directory_path.strip():
        return False, "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
    
    try:
        path = Path(directory_path)
        
        # ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèª
        if not path.exists():
            return False, f"æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {directory_path}"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã©ã†ã‹ã®ç¢ºèª
        if not path.is_dir():
            return False, f"æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“: {directory_path}"
        
        # æ›¸ãè¾¼ã¿æ¨©é™ã®ç¢ºèª
        if not os.access(path, os.W_OK):
            return False, f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ›¸ãè¾¼ã¿æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: {directory_path}"
        
        # èª­ã¿å–ã‚Šæ¨©é™ã®ç¢ºèª
        if not os.access(path, os.R_OK):
            return False, f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«èª­ã¿å–ã‚Šæ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: {directory_path}"
        
        return True, f"âœ… æœ‰åŠ¹ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã™: {path.absolute()}"
        
    except Exception as e:
        return False, f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}"


def display_page_list_and_preview(bookmarks: List[Bookmark], duplicates: Dict, output_directory: Path):
    """
    Task 9: ãƒšãƒ¼ã‚¸ä¸€è¦§è¡¨ç¤ºã¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½
    
    Args:
        bookmarks: ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä¸€è¦§
        duplicates: é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
        output_directory: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    st.header("ğŸ“‹ ãƒšãƒ¼ã‚¸ä¸€è¦§ã¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
    st.markdown("å‡¦ç†å¯¾è±¡ã®ãƒšãƒ¼ã‚¸ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤ºã§ãã¾ã™ã€‚")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if 'selected_pages' not in st.session_state:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å…¨ã¦ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ï¼ˆé‡è¤‡é™¤å¤–ï¼‰
        st.session_state.selected_pages = {}
        for i, bookmark in enumerate(bookmarks):
            # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã§ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒã‚§ãƒƒã‚¯
            is_duplicate = any(bookmark.title in dup_file for dup_file in duplicates.get('files', []))
            st.session_state.selected_pages[i] = not is_duplicate
    
    if 'preview_cache' not in st.session_state:
        st.session_state.preview_cache = {}
    
    # å…¨é¸æŠ/å…¨è§£é™¤ãƒœã‚¿ãƒ³
    col1, col2, col3 = st.columns([1, 1, 4])
    
    with col1:
        if st.button("âœ… å…¨é¸æŠ"):
            for i in range(len(bookmarks)):
                st.session_state.selected_pages[i] = True
            st.rerun()
    
    with col2:
        if st.button("âŒ å…¨è§£é™¤"):
            for i in range(len(bookmarks)):
                st.session_state.selected_pages[i] = False
            st.rerun()
    
    with col3:
        selected_count = sum(1 for selected in st.session_state.selected_pages.values() if selected)
        st.write(f"**é¸æŠä¸­:** {selected_count}/{len(bookmarks)} ãƒšãƒ¼ã‚¸")
    
    # ãƒ•ã‚©ãƒ«ãƒ€åˆ¥ã«ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚’æ•´ç†
    folder_groups = organize_bookmarks_by_folder(bookmarks)
    
    # å±•é–‹å¯èƒ½ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ„ãƒªãƒ¼å½¢å¼ã§è¡¨ç¤º
    for folder_path, folder_bookmarks in folder_groups.items():
        folder_name = ' > '.join(folder_path) if folder_path else "ğŸ“ ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€"
        
        # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®é¸æŠçŠ¶æ³ã‚’è¨ˆç®—
        folder_indices = [bookmarks.index(bookmark) for bookmark in folder_bookmarks]
        folder_selected = sum(1 for idx in folder_indices if st.session_state.selected_pages.get(idx, False))
        
        with st.expander(f"ğŸ“‚ {folder_name} ({folder_selected}/{len(folder_bookmarks)} é¸æŠ)", expanded=True):
            for bookmark in folder_bookmarks:
                original_index = bookmarks.index(bookmark)
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                is_duplicate = any(bookmark.title in dup_file for dup_file in duplicates.get('files', []))
                
                # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã¨ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’è¡¨ç¤º
                col1, col2, col3 = st.columns([1, 4, 1])
                
                with col1:
                    if is_duplicate:
                        st.checkbox(
                            "é‡è¤‡",
                            value=False,
                            disabled=True,
                            key=f"checkbox_dup_{original_index}",
                            help="ã“ã®ãƒšãƒ¼ã‚¸ã¯æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¨é‡è¤‡ã—ã¦ã„ã¾ã™"
                        )
                        st.session_state.selected_pages[original_index] = False
                    else:
                        selected = st.checkbox(
                            "é¸æŠ",
                            value=st.session_state.selected_pages.get(original_index, True),
                            key=f"checkbox_{original_index}"
                        )
                        st.session_state.selected_pages[original_index] = selected
                
                with col2:
                    # ãƒšãƒ¼ã‚¸æƒ…å ±è¡¨ç¤º
                    if is_duplicate:
                        st.markdown(f"~~**{bookmark.title}**~~ *(é‡è¤‡)*")
                    else:
                        st.markdown(f"**{bookmark.title}**")
                    
                    st.markdown(f"ğŸ”— [{bookmark.url}]({bookmark.url})")
                    
                    if bookmark.add_date:
                        st.caption(f"ğŸ“… {bookmark.add_date.strftime('%Y-%m-%d %H:%M:%S')}")
                
                with col3:
                    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒœã‚¿ãƒ³
                    if not is_duplicate:
                        if st.button("ğŸ‘ï¸ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", key=f"preview_{original_index}"):
                            show_page_preview(bookmark, original_index)
                    else:
                        st.caption("é‡è¤‡ã«ã‚ˆã‚Šé™¤å¤–")
                
                st.divider()
    
    # ä¿å­˜ãƒœã‚¿ãƒ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.markdown("---")
    st.subheader("ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜")
    
    selected_bookmarks = [
        bookmark for i, bookmark in enumerate(bookmarks) 
        if st.session_state.selected_pages.get(i, False)
    ]
    
    if selected_bookmarks:
        col1, col2 = st.columns([1, 3])
        
        with col1:
            if st.button("ğŸ’¾ é¸æŠã—ãŸãƒšãƒ¼ã‚¸ã‚’ä¿å­˜", type="primary"):
                save_selected_pages(selected_bookmarks, output_directory)
        
        with col2:
            st.info(f"ğŸ’¡ {len(selected_bookmarks)}å€‹ã®ãƒšãƒ¼ã‚¸ãŒMarkdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã™")
            st.caption(f"ä¿å­˜å…ˆ: {output_directory}")
    else:
        st.warning("âš ï¸ ä¿å­˜ã™ã‚‹ãƒšãƒ¼ã‚¸ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")


def organize_bookmarks_by_folder(bookmarks: List[Bookmark]) -> Dict[tuple, List[Bookmark]]:
    """
    ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚’ãƒ•ã‚©ãƒ«ãƒ€åˆ¥ã«æ•´ç†
    
    Args:
        bookmarks: ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä¸€è¦§
        
    Returns:
        Dict[tuple, List[Bookmark]]: ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’ã‚­ãƒ¼ã¨ã—ãŸãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è¾æ›¸
    """
    folder_groups = {}
    
    for bookmark in bookmarks:
        folder_key = tuple(bookmark.folder_path) if bookmark.folder_path else ()
        
        if folder_key not in folder_groups:
            folder_groups[folder_key] = []
        
        folder_groups[folder_key].append(bookmark)
    
    # ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã§ã‚½ãƒ¼ãƒˆ
    return dict(sorted(folder_groups.items()))


def show_page_preview(bookmark: Bookmark, index: int):
    """
    Task 10: é€²æ—è¡¨ç¤ºã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å¼·åŒ–ã—ãŸãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½
    
    Args:
        bookmark: ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æƒ…å ±
        index: ãƒšãƒ¼ã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if index not in st.session_state.preview_cache:
        
        # é€²æ—è¡¨ç¤ºã‚³ãƒ³ãƒ†ãƒŠ
        progress_container = st.container()
        
        with progress_container:
            st.info(f"ğŸ” ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—ä¸­: {bookmark.title}")
            
            # è©³ç´°é€²æ—è¡¨ç¤º
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒšãƒ¼ã‚¸å–å¾—
                status_text.text("ğŸŒ ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—ä¸­...")
                progress_bar.progress(0.2)
                
                scraper = WebScraper()
                html_content = None
                
                try:
                    html_content = scraper.fetch_page_content(bookmark.url)
                except requests.exceptions.ConnectionError:
                    st.session_state.preview_cache[index] = {
                        'success': False,
                        'error': 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼',
                        'error_type': 'network',
                        'retryable': True
                    }
                    error_logger.log_error(bookmark, 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼', 'network', True)
                    return
                except requests.exceptions.Timeout:
                    st.session_state.preview_cache[index] = {
                        'success': False,
                        'error': 'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼',
                        'error_type': 'timeout',
                        'retryable': True
                    }
                    error_logger.log_error(bookmark, 'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼', 'timeout', True)
                    return
                except Exception as e:
                    st.session_state.preview_cache[index] = {
                        'success': False,
                        'error': f'ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}',
                        'error_type': 'fetch',
                        'retryable': False
                    }
                    error_logger.log_error(bookmark, f'ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}', 'fetch', False)
                    return
                
                # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
                status_text.text("ğŸ“„ è¨˜äº‹å†…å®¹ã‚’æŠ½å‡ºä¸­...")
                progress_bar.progress(0.6)
                
                article_data = None
                if html_content:
                    try:
                        article_data = scraper.extract_article_content(html_content, bookmark.url)
                    except Exception as e:
                        logger.warning(f"âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)} - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨")
                        error_logger.log_error(bookmark, f'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}', 'extraction', False)
                
                # ã‚¹ãƒ†ãƒƒãƒ—3: Markdownç”Ÿæˆ
                status_text.text("ğŸ“ Markdownã‚’ç”Ÿæˆä¸­...")
                progress_bar.progress(0.8)
                
                try:
                    generator = MarkdownGenerator()
                    if article_data:
                        markdown_content = generator.generate_obsidian_markdown(article_data, bookmark)
                    else:
                        markdown_content = generator._generate_fallback_markdown(bookmark)
                        article_data = {
                            'title': bookmark.title,
                            'content': 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ',
                            'quality_score': 0.0,
                            'extraction_method': 'fallback',
                            'tags': []
                        }
                except Exception as e:
                    st.session_state.preview_cache[index] = {
                        'success': False,
                        'error': f'Markdownç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}',
                        'error_type': 'markdown',
                        'retryable': False
                    }
                    error_logger.log_error(bookmark, f'Markdownç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}', 'markdown', False)
                    return
                
                # ã‚¹ãƒ†ãƒƒãƒ—4: å®Œäº†
                status_text.text("âœ… ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æº–å‚™å®Œäº†")
                progress_bar.progress(1.0)
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                st.session_state.preview_cache[index] = {
                    'success': True,
                    'article_data': article_data,
                    'markdown': markdown_content,
                    'fetch_time': datetime.datetime.now()
                }
                
                # é€²æ—è¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢
                progress_container.empty()
                
            except Exception as e:
                st.session_state.preview_cache[index] = {
                    'success': False,
                    'error': f'äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}',
                    'error_type': 'unexpected',
                    'retryable': False
                }
                error_logger.log_error(bookmark, f'äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}', 'unexpected', False)
                progress_container.empty()
                return
    
    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
    preview_data = st.session_state.preview_cache[index]
    
    if preview_data['success']:
        article_data = preview_data['article_data']
        
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
        st.subheader(f"ğŸ“„ {bookmark.title} - ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±
        if 'fetch_time' in preview_data:
            fetch_time = preview_data['fetch_time']
            st.caption(f"ğŸ•’ å–å¾—æ™‚åˆ»: {fetch_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # åŸºæœ¬æƒ…å ±
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown(f"**URL:** {bookmark.url}")
            quality_score = article_data.get('quality_score', 'N/A')
            if isinstance(quality_score, (int, float)):
                quality_color = "ğŸŸ¢" if quality_score > 0.7 else "ğŸŸ¡" if quality_score > 0.4 else "ğŸ”´"
                st.markdown(f"**å“è³ªã‚¹ã‚³ã‚¢:** {quality_color} {quality_score}")
            else:
                st.markdown(f"**å“è³ªã‚¹ã‚³ã‚¢:** {quality_score}")
        
        with col2:
            extraction_method = article_data.get('extraction_method', 'N/A')
            method_icon = "âœ…" if extraction_method != 'fallback' else "âš ï¸"
            st.markdown(f"**æŠ½å‡ºæ–¹æ³•:** {method_icon} {extraction_method}")
            
            content_length = len(article_data.get('content', ''))
            st.markdown(f"**æ–‡å­—æ•°:** {content_length:,}æ–‡å­—")
        
        # ã‚¿ã‚°è¡¨ç¤º
        if article_data.get('tags'):
            st.markdown("**ã‚¿ã‚°:** " + ", ".join([f"`{tag}`" for tag in article_data['tags']]))
        
        # è¨˜äº‹å†…å®¹ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰
        content = article_data.get('content', '')
        if content:
            st.markdown("**è¨˜äº‹å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:**")
            preview_content = content[:500] + "..." if len(content) > 500 else content
            st.text_area("å†…å®¹", preview_content, height=200, disabled=True)
        
        # ç”Ÿæˆã•ã‚Œã‚‹Markdownã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        with st.expander("ğŸ“ ç”Ÿæˆã•ã‚Œã‚‹Markdownãƒ•ã‚¡ã‚¤ãƒ«"):
            st.code(preview_data['markdown'], language='markdown')
        
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ”„ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ›´æ–°", key=f"refresh_preview_{index}"):
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†å–å¾—
                if index in st.session_state.preview_cache:
                    del st.session_state.preview_cache[index]
                st.rerun()
        
        with col2:
            if st.button("ğŸ“‹ URLã‚’ã‚³ãƒ”ãƒ¼", key=f"copy_url_{index}"):
                st.code(bookmark.url)
                st.success("URLã‚’è¡¨ç¤ºã—ã¾ã—ãŸ")
    
    else:
        error_type = preview_data.get('error_type', 'unknown')
        retryable = preview_data.get('retryable', False)
        
        # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã‚¢ã‚¤ã‚³ãƒ³ã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        error_icons = {
            'network': 'ğŸ”Œ',
            'timeout': 'â°',
            'fetch': 'ğŸŒ',
            'extraction': 'ğŸ“„',
            'markdown': 'ğŸ“',
            'unexpected': 'ğŸ’¥'
        }
        
        error_icon = error_icons.get(error_type, 'âŒ')
        st.error(f"{error_icon} ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ©ãƒ¼: {preview_data['error']}")
        
        if retryable:
            st.info("ğŸ”„ ã“ã®ã‚¨ãƒ©ãƒ¼ã¯ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ã§ã™")
            if st.button("ğŸ”„ ãƒªãƒˆãƒ©ã‚¤", key=f"retry_preview_{index}"):
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†å–å¾—
                if index in st.session_state.preview_cache:
                    del st.session_state.preview_cache[index]
                st.rerun()
        else:
            st.info("ğŸ’¡ ã“ã®ãƒšãƒ¼ã‚¸ã¯æ‰‹å‹•ã§ç¢ºèªãŒå¿…è¦ã§ã™")
        
        # ã‚¨ãƒ©ãƒ¼è©³ç´°æƒ…å ±
        with st.expander("ğŸ” ã‚¨ãƒ©ãƒ¼è©³ç´°"):
            st.write(f"**ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—:** {error_type}")
            st.write(f"**ãƒªãƒˆãƒ©ã‚¤å¯èƒ½:** {'ã¯ã„' if retryable else 'ã„ã„ãˆ'}")
            st.write(f"**URL:** {bookmark.url}")
            st.write(f"**ã‚¿ã‚¤ãƒˆãƒ«:** {bookmark.title}")


def save_selected_pages(selected_bookmarks: List[Bookmark], output_directory: Path):
    """
    Task 10: é€²æ—è¡¨ç¤ºã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ©Ÿèƒ½ã‚’å¼·åŒ–ã—ãŸä¿å­˜æ©Ÿèƒ½
    
    Args:
        selected_bookmarks: é¸æŠã•ã‚ŒãŸãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ä¸€è¦§
        output_directory: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    if not selected_bookmarks:
        st.warning("ä¿å­˜ã™ã‚‹ãƒšãƒ¼ã‚¸ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    # é€²æ—è¡¨ç¤ºã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®åˆæœŸåŒ–
    progress_container = st.container()
    error_container = st.container()
    
    with progress_container:
        st.subheader("ğŸ“Š å‡¦ç†é€²æ—")
        
        # è¤‡æ•°ã®é€²æ—ãƒãƒ¼
        overall_progress = st.progress(0)
        current_progress = st.progress(0)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
        status_text = st.empty()
        current_task = st.empty()
        
        # çµ±è¨ˆæƒ…å ±
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            success_metric = st.metric("âœ… æˆåŠŸ", 0)
        with col2:
            error_metric = st.metric("âŒ ã‚¨ãƒ©ãƒ¼", 0)
        with col3:
            skip_metric = st.metric("â­ï¸ ã‚¹ã‚­ãƒƒãƒ—", 0)
        with col4:
            remaining_metric = st.metric("â³ æ®‹ã‚Š", len(selected_bookmarks))
    
    # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã¨ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
    error_log = []
    retry_queue = []
    
    scraper = WebScraper()
    generator = MarkdownGenerator()
    
    saved_count = 0
    error_count = 0
    skip_count = 0
    
    # ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ«ãƒ¼ãƒ—
    for i, bookmark in enumerate(selected_bookmarks):
        overall_progress_value = (i + 1) / len(selected_bookmarks)
        overall_progress.progress(overall_progress_value)
        
        status_text.text(f"ğŸ“‹ å‡¦ç†ä¸­: {i+1}/{len(selected_bookmarks)} ãƒšãƒ¼ã‚¸")
        current_task.text(f"ğŸ” ç¾åœ¨ã®å‡¦ç†: {bookmark.title}")
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒšãƒ¼ã‚¸å†…å®¹å–å¾—
            current_progress.progress(0.2)
            current_task.text(f"ğŸŒ ãƒšãƒ¼ã‚¸å–å¾—ä¸­: {bookmark.title}")
            
            html_content = None
            article_data = None
            
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            try:
                html_content = scraper.fetch_page_content(bookmark.url)
            except requests.exceptions.ConnectionError:
                error_msg = f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼: {bookmark.url}"
                error_log.append({
                    'bookmark': bookmark,
                    'error': error_msg,
                    'type': 'network',
                    'retryable': True
                })
                logger.error(f"ğŸ”Œ {error_msg}")
                skip_count += 1
                continue
            except requests.exceptions.Timeout:
                error_msg = f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼: {bookmark.url}"
                error_log.append({
                    'bookmark': bookmark,
                    'error': error_msg,
                    'type': 'timeout',
                    'retryable': True
                })
                logger.error(f"â° {error_msg}")
                skip_count += 1
                continue
            except Exception as e:
                error_msg = f"ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"
                error_log.append({
                    'bookmark': bookmark,
                    'error': error_msg,
                    'type': 'fetch',
                    'retryable': False
                })
                logger.error(f"âŒ {error_msg}")
                error_count += 1
                continue
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
            current_progress.progress(0.5)
            current_task.text(f"ğŸ“„ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºä¸­: {bookmark.title}")
            
            if html_content:
                try:
                    article_data = scraper.extract_article_content(html_content, bookmark.url)
                except Exception as e:
                    error_msg = f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}"
                    error_log.append({
                        'bookmark': bookmark,
                        'error': error_msg,
                        'type': 'extraction',
                        'retryable': False
                    })
                    logger.warning(f"âš ï¸ {error_msg} - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨")
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: Markdownç”Ÿæˆ
            current_progress.progress(0.7)
            current_task.text(f"ğŸ“ Markdownç”Ÿæˆä¸­: {bookmark.title}")
            
            try:
                if article_data:
                    markdown_content = generator.generate_obsidian_markdown(article_data, bookmark)
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨Markdown
                    markdown_content = generator._generate_fallback_markdown(bookmark)
            except Exception as e:
                error_msg = f"Markdownç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
                error_log.append({
                    'bookmark': bookmark,
                    'error': error_msg,
                    'type': 'markdown',
                    'retryable': False
                })
                logger.error(f"âŒ {error_msg}")
                error_count += 1
                continue
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            current_progress.progress(0.9)
            current_task.text(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­: {bookmark.title}")
            
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
                file_path = generator.generate_file_path(bookmark, output_directory)
                
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
                file_path.parent.mkdir(parents=True, exist_ok=True)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                
                saved_count += 1
                logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æˆåŠŸ: {file_path}")
                
            except PermissionError:
                error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æ¨©é™ã‚¨ãƒ©ãƒ¼: {file_path}"
                error_log.append({
                    'bookmark': bookmark,
                    'error': error_msg,
                    'type': 'permission',
                    'retryable': False
                })
                logger.error(f"ğŸ”’ {error_msg}")
                error_count += 1
                continue
            except OSError as e:
                error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {str(e)}"
                error_log.append({
                    'bookmark': bookmark,
                    'error': error_msg,
                    'type': 'filesystem',
                    'retryable': False
                })
                logger.error(f"ğŸ’¾ {error_msg}")
                error_count += 1
                continue
            except Exception as e:
                error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}"
                error_log.append({
                    'bookmark': bookmark,
                    'error': error_msg,
                    'type': 'save',
                    'retryable': False
                })
                logger.error(f"âŒ {error_msg}")
                error_count += 1
                continue
            
            # ã‚¹ãƒ†ãƒƒãƒ—5: å®Œäº†
            current_progress.progress(1.0)
            
        except Exception as e:
            # äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼
            error_msg = f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}"
            error_log.append({
                'bookmark': bookmark,
                'error': error_msg,
                'type': 'unexpected',
                'retryable': False
            })
            logger.error(f"ğŸ’¥ {error_msg}")
            error_count += 1
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
        with col1:
            success_metric.metric("âœ… æˆåŠŸ", saved_count)
        with col2:
            error_metric.metric("âŒ ã‚¨ãƒ©ãƒ¼", error_count)
        with col3:
            skip_metric.metric("â­ï¸ ã‚¹ã‚­ãƒƒãƒ—", skip_count)
        with col4:
            remaining_metric.metric("â³ æ®‹ã‚Š", len(selected_bookmarks) - i - 1)
    
    # å®Œäº†å‡¦ç†
    overall_progress.progress(1.0)
    current_progress.progress(1.0)
    status_text.text("ğŸ‰ å‡¦ç†å®Œäº†ï¼")
    current_task.text("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
    
    # çµæœã‚µãƒãƒªãƒ¼
    st.markdown("---")
    st.subheader("ğŸ“Š å‡¦ç†çµæœã‚µãƒãƒªãƒ¼")
    
    total_processed = saved_count + error_count + skip_count
    
    if saved_count > 0:
        st.success(f"âœ… {saved_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    if error_count > 0:
        st.error(f"âŒ {error_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    
    if skip_count > 0:
        st.warning(f"â­ï¸ {skip_count}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
    
    # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®è¡¨ç¤º
    if error_log:
        with error_container:
            st.subheader("ğŸš¨ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°")
            
            # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®é›†è¨ˆ
            error_types = {}
            retryable_errors = []
            
            for error in error_log:
                error_type = error['type']
                if error_type not in error_types:
                    error_types[error_type] = 0
                error_types[error_type] += 1
                
                if error['retryable']:
                    retryable_errors.append(error)
            
            # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥è¡¨ç¤º
            st.markdown("**ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ:**")
            for error_type, count in error_types.items():
                error_type_names = {
                    'network': 'ğŸ”Œ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼',
                    'timeout': 'â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼',
                    'fetch': 'ğŸŒ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼',
                    'extraction': 'ğŸ“„ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼',
                    'markdown': 'ğŸ“ Markdownç”Ÿæˆã‚¨ãƒ©ãƒ¼',
                    'permission': 'ğŸ”’ æ¨©é™ã‚¨ãƒ©ãƒ¼',
                    'filesystem': 'ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼',
                    'save': 'ğŸ’¾ ä¿å­˜ã‚¨ãƒ©ãƒ¼',
                    'unexpected': 'ğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼'
                }
                st.write(f"- {error_type_names.get(error_type, error_type)}: {count}ä»¶")
            
            # è©³ç´°ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
            with st.expander("ğŸ“‹ è©³ç´°ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°"):
                for i, error in enumerate(error_log):
                    st.write(f"**{i+1}. {error['bookmark'].title}**")
                    st.write(f"   URL: {error['bookmark'].url}")
                    st.write(f"   ã‚¨ãƒ©ãƒ¼: {error['error']}")
                    st.write(f"   ã‚¿ã‚¤ãƒ—: {error['type']}")
                    if error['retryable']:
                        st.write("   ğŸ”„ ãƒªãƒˆãƒ©ã‚¤å¯èƒ½")
                    st.write("---")
            
            # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
            if retryable_errors:
                st.subheader("ğŸ”„ ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½")
                st.info(f"{len(retryable_errors)}å€‹ã®ã‚¨ãƒ©ãƒ¼ã¯ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ã§ã™")
                
                if st.button("ğŸ”„ ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒˆãƒ©ã‚¤"):
                    retry_bookmarks = [error['bookmark'] for error in retryable_errors]
                    st.info("ãƒªãƒˆãƒ©ã‚¤ã‚’é–‹å§‹ã—ã¾ã™...")
                    save_selected_pages(retry_bookmarks, output_directory)
    
    # ä¿å­˜å…ˆæƒ…å ±
    st.info(f"ğŸ“ ä¿å­˜å…ˆ: {output_directory}")
    
    # å‡¦ç†å®Œäº†ãƒ­ã‚°
    logger.info(f"ğŸ‰ å‡¦ç†å®Œäº†: æˆåŠŸ={saved_count}, ã‚¨ãƒ©ãƒ¼={error_count}, ã‚¹ã‚­ãƒƒãƒ—={skip_count}")


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°"""
    # ãƒšãƒ¼ã‚¸è¨­å®š
    st.set_page_config(
        page_title="Bookmark to Obsidian Converter",
        page_icon="ğŸ“š",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
    st.title("ğŸ“š Bookmark to Obsidian Converter")
    st.markdown("---")
    
    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èª¬æ˜
    st.markdown("""
    ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€Google Chromeã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆbookmarks.htmlï¼‰ã‚’è§£æã—ã€
    ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã•ã‚ŒãŸWebãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—ã—ã¦Obsidianç”¨ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚
    """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("ğŸ”§ è¨­å®š")
        st.markdown("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé¸æŠ")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
        st.subheader("ğŸ“ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«")
        uploaded_file = st.file_uploader(
            "bookmarks.htmlãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
            type=['html'],
            help="Google Chromeã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆbookmarks.htmlï¼‰ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼çµæœã®è¡¨ç¤º
        if uploaded_file is not None:
            logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: {uploaded_file.name} (ã‚µã‚¤ã‚º: {uploaded_file.size} bytes)")
            is_valid_file, file_message = validate_bookmarks_file(uploaded_file)
            if is_valid_file:
                st.success(file_message)
                logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼æˆåŠŸ: {file_message}")
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                st.session_state['uploaded_file'] = uploaded_file
                st.session_state['file_validated'] = True
            else:
                st.error(file_message)
                logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼å¤±æ•—: {file_message}")
                st.session_state['file_validated'] = False
        else:
            st.session_state['file_validated'] = False
        
        st.markdown("---")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé¸æŠæ©Ÿèƒ½
        st.subheader("ğŸ“‚ ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã®ææ¡ˆ
        default_path = str(Path.home() / "Documents" / "Obsidian")
        
        directory_path = st.text_input(
            "Obsidianãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value=default_path,
            help="Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
        )
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¤œè¨¼çµæœã®è¡¨ç¤º
        if directory_path:
            logger.info(f"ğŸ“‚ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæŒ‡å®š: {directory_path}")
            is_valid_dir, dir_message = validate_directory_path(directory_path)
            if is_valid_dir:
                st.success(dir_message)
                logger.info(f"âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¤œè¨¼æˆåŠŸ: {directory_path}")
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ä¿å­˜
                st.session_state['output_directory'] = Path(directory_path)
                st.session_state['directory_validated'] = True
            else:
                st.error(dir_message)
                logger.error(f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¤œè¨¼å¤±æ•—: {dir_message}")
                st.session_state['directory_validated'] = False
        else:
            st.session_state['directory_validated'] = False
        
        st.markdown("---")
        
        # è¨­å®šçŠ¶æ³ã®è¡¨ç¤º
        st.subheader("âš™ï¸ è¨­å®šçŠ¶æ³")
        file_status = "âœ… å®Œäº†" if st.session_state.get('file_validated', False) else "âŒ æœªå®Œäº†"
        dir_status = "âœ… å®Œäº†" if st.session_state.get('directory_validated', False) else "âŒ æœªå®Œäº†"
        
        st.write(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ: {file_status}")
        st.write(f"ğŸ“‚ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé¸æŠ: {dir_status}")
        
        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®æº–å‚™çŠ¶æ³
        ready_to_proceed = (
            st.session_state.get('file_validated', False) and 
            st.session_state.get('directory_validated', False)
        )
        
        if ready_to_proceed:
            st.success("ğŸš€ è§£æã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸï¼")
            
            # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æãƒœã‚¿ãƒ³
            if st.button("ğŸ“Š ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æã‚’é–‹å§‹", type="primary"):
                st.session_state['start_analysis'] = True
        else:
            st.info("ğŸ“‹ ä¸Šè¨˜ã®è¨­å®šã‚’å®Œäº†ã—ã¦ãã ã•ã„")
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ“‹ å‡¦ç†æ‰‹é †")
        
        # è¨­å®šçŠ¶æ³ã«å¿œã˜ãŸæ‰‹é †è¡¨ç¤º
        ready_to_proceed = (
            st.session_state.get('file_validated', False) and 
            st.session_state.get('directory_validated', False)
        )
        
        if ready_to_proceed:
            # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æã®å®Ÿè¡Œ
            if st.session_state.get('start_analysis', False):
                st.markdown("### ğŸ“Š ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æçµæœ")
                
                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿å–ã‚Š
                    uploaded_file = st.session_state['uploaded_file']
                    content = uploaded_file.read().decode('utf-8')
                    uploaded_file.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
                    
                    # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æã®å®Ÿè¡Œ
                    with st.spinner("ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚’è§£æä¸­..."):
                        logger.info("ğŸ“Š ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æã‚’é–‹å§‹...")
                        parser = BookmarkParser()
                        bookmarks = parser.parse_bookmarks(content)
                        logger.info(f"ğŸ“š ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æå®Œäº†: {len(bookmarks)}å€‹ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚’æ¤œå‡º")
                        
                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                        st.session_state['bookmarks'] = bookmarks
                        st.session_state['parser'] = parser
                        
                        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç®¡ç†ã®åˆæœŸåŒ–ã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        output_directory = st.session_state['output_directory']
                        logger.info(f"ğŸ“‚ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹: {output_directory}")
                        directory_manager = LocalDirectoryManager(output_directory)
                        
                        # æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ã‚¹ã‚­ãƒ£ãƒ³
                        existing_structure = directory_manager.scan_directory()
                        logger.info(f"ğŸ“ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {sum(len(files) for files in existing_structure.values())}å€‹ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«")
                        
                        # æ—¢å­˜æ§‹é€ ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
                        for path, files in existing_structure.items():
                            path_display = path if path else "(ãƒ«ãƒ¼ãƒˆ)"
                            logger.info(f"  ğŸ“ {path_display}: {files}")
                        
                        # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        logger.info("ğŸ”„ é‡è¤‡ãƒã‚§ãƒƒã‚¯é–‹å§‹...")
                        duplicates = directory_manager.compare_with_bookmarks(bookmarks)
                        logger.info(f"ğŸ”„ é‡è¤‡ãƒã‚§ãƒƒã‚¯å®Œäº†: {len(duplicates['files'])}å€‹ã®é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º")
                        
                        # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
                        if duplicates['files']:
                            logger.info("é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
                            for duplicate in duplicates['files']:
                                logger.info(f"  ğŸ”„ {duplicate}")
                        
                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                        st.session_state['directory_manager'] = directory_manager
                        st.session_state['existing_structure'] = existing_structure
                        st.session_state['duplicates'] = duplicates
                    
                    # è§£æçµæœã®è¡¨ç¤º
                    if bookmarks:
                        stats = parser.get_statistics(bookmarks)
                        
                        # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
                        directory_manager = st.session_state['directory_manager']
                        dir_stats = directory_manager.get_statistics()
                        duplicates = st.session_state['duplicates']
                        
                        logger.info("ğŸ“Š çµ±è¨ˆæƒ…å ±:")
                        logger.info(f"  ğŸ“š ç·ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°: {stats['total_bookmarks']}")
                        logger.info(f"  ğŸŒ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°: {stats['unique_domains']}")
                        logger.info(f"  ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€æ•°: {stats['folder_count']}")
                        logger.info(f"  ğŸ”„ é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(duplicates['files'])}")
                        
                        col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
                        with col_stat1:
                            st.metric("ğŸ“š ç·ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°", stats['total_bookmarks'])
                        with col_stat2:
                            st.metric("ğŸŒ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°", stats['unique_domains'])
                        with col_stat3:
                            st.metric("ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€æ•°", stats['folder_count'])
                        with col_stat4:
                            st.metric("ğŸ”„ é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ•°", len(duplicates['files']))
                        
                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœã®è¡¨ç¤º
                        st.subheader("ğŸ”„ é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœ")
                        existing_structure = st.session_state['existing_structure']
                        
                        if existing_structure:
                            st.info(f"ğŸ“‚ æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ {dir_stats['total_files']} å€‹ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
                            
                            if duplicates['files']:
                                st.warning(f"âš ï¸ {len(duplicates['files'])} å€‹ã®é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                                
                                with st.expander("é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º"):
                                    for duplicate_file in duplicates['files'][:20]:  # æœ€åˆã®20å€‹ã‚’è¡¨ç¤º
                                        st.write(f"  - ğŸ”„ {duplicate_file}")
                                    if len(duplicates['files']) > 20:
                                        st.write(f"  ... ä»– {len(duplicates['files']) - 20}å€‹")
                                
                                st.info("ğŸ’¡ é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•çš„ã«å‡¦ç†å¯¾è±¡ã‹ã‚‰é™¤å¤–ã•ã‚Œã¾ã™")
                            else:
                                st.success("âœ… é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        else:
                            st.info("ğŸ“‚ ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ç©ºã§ã™ï¼ˆæ–°è¦ä½œæˆï¼‰")
                        
                        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®è¡¨ç¤º
                        st.subheader("ğŸ“‚ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ§‹é€ ")
                        directory_structure = parser.extract_directory_structure(bookmarks)
                        
                        # å‡¦ç†å¯¾è±¡ã¨é™¤å¤–å¯¾è±¡ã‚’åˆ†ã‘ã¦è¡¨ç¤º
                        total_to_process = 0
                        total_excluded = 0
                        
                        for folder_path, filenames in directory_structure.items():
                            # ã“ã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã®é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’è¨ˆç®—
                            if folder_path:
                                folder_duplicates = [f for f in duplicates['files'] 
                                                   if f.startswith(folder_path + '/')]
                            else:
                                folder_duplicates = [f for f in duplicates['files'] 
                                                   if '/' not in f]
                            
                            excluded_count = len([f for f in filenames 
                                                if directory_manager.check_file_exists(folder_path, f)])
                            process_count = len(filenames) - excluded_count
                            
                            total_to_process += process_count
                            total_excluded += excluded_count
                            
                            if folder_path:
                                status_text = f"ğŸ“ {folder_path}"
                                if excluded_count > 0:
                                    status_text += f" ({process_count}å€‹å‡¦ç†äºˆå®š, {excluded_count}å€‹é™¤å¤–)"
                                else:
                                    status_text += f" ({process_count}å€‹å‡¦ç†äºˆå®š)"
                                st.write(f"**{status_text}**")
                            else:
                                status_text = f"ğŸ“„ ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
                                if excluded_count > 0:
                                    status_text += f" ({process_count}å€‹å‡¦ç†äºˆå®š, {excluded_count}å€‹é™¤å¤–)"
                                else:
                                    status_text += f" ({process_count}å€‹å‡¦ç†äºˆå®š)"
                                st.write(f"**{status_text}**")
                        
                        # å‡¦ç†äºˆå®šã®çµ±è¨ˆã‚’è¡¨ç¤º
                        st.markdown("---")
                        col_process1, col_process2 = st.columns(2)
                        with col_process1:
                            st.metric("âœ… å‡¦ç†äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«", total_to_process)
                        with col_process2:
                            st.metric("ğŸ”„ é™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«", total_excluded)
                        
                        # ã‚µãƒ³ãƒ—ãƒ«ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã®è¡¨ç¤º
                        st.subheader("ğŸ“‹ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚µãƒ³ãƒ—ãƒ«")
                        sample_bookmarks = bookmarks[:5]  # æœ€åˆã®5å€‹ã‚’è¡¨ç¤º
                        
                        for i, bookmark in enumerate(sample_bookmarks):
                            with st.expander(f"{i+1}. {bookmark.title}"):
                                st.write(f"**URL:** {bookmark.url}")
                                st.write(f"**ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹:** {' > '.join(bookmark.folder_path) if bookmark.folder_path else 'ãƒ«ãƒ¼ãƒˆ'}")
                                if bookmark.add_date:
                                    st.write(f"**è¿½åŠ æ—¥:** {bookmark.add_date.strftime('%Y-%m-%d %H:%M:%S')}")
                        
                        st.success(f"âœ… ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                        st.info(f"ğŸ“Š {len(bookmarks)}å€‹ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã€{total_to_process}å€‹ãŒå‡¦ç†å¯¾è±¡ã€{total_excluded}å€‹ãŒé‡è¤‡ã«ã‚ˆã‚Šé™¤å¤–ã•ã‚Œã¾ã—ãŸã€‚")
                        
                        # Task 9: ãƒšãƒ¼ã‚¸ä¸€è¦§è¡¨ç¤ºã¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½
                        if total_to_process > 0:
                            st.markdown("---")
                            display_page_list_and_preview(bookmarks, duplicates, st.session_state['output_directory'])
                        
                        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
                        if len(duplicates['files']) == 0 and len(existing_structure) > 0:
                            st.warning("âš ï¸ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã®ã«é‡è¤‡ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                            
                            with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±"):
                                st.write("**æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ä¾‹ï¼ˆæœ€åˆã®5å€‹ï¼‰:**")
                                file_count = 0
                                for path, files in existing_structure.items():
                                    for file in files:
                                        if file_count >= 5:
                                            break
                                        path_display = path if path else "(ãƒ«ãƒ¼ãƒˆ)"
                                        st.write(f"- {path_display}/{file}")
                                        file_count += 1
                                    if file_count >= 5:
                                        break
                                
                                st.write("**ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚¿ã‚¤ãƒˆãƒ«ä¾‹ï¼ˆæœ€åˆã®5å€‹ï¼‰:**")
                                for i, bookmark in enumerate(bookmarks[:5]):
                                    folder_display = " > ".join(bookmark.folder_path) if bookmark.folder_path else "(ãƒ«ãƒ¼ãƒˆ)"
                                    st.write(f"- {folder_display}/{bookmark.title}")
                                
                                st.write("**ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«åä¾‹:**")
                                for i, bookmark in enumerate(bookmarks[:5]):
                                    sanitized = parser._sanitize_filename(bookmark.title)
                                    st.write(f"- '{bookmark.title}' â†’ '{sanitized}'")
                        
                    else:
                        st.warning("âš ï¸ æœ‰åŠ¹ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                        
                except Exception as e:
                    st.error(f"âŒ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    st.session_state['start_analysis'] = False
            
            else:
                st.markdown("""
                âœ… **ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: å®Œäº†  
                âœ… **ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé¸æŠ**: å®Œäº†  
                
                **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:**
                3. **ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æ**: ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã¨URLã‚’è§£æ â† ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
                4. **é‡è¤‡ãƒã‚§ãƒƒã‚¯**: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®é‡è¤‡ã‚’ç¢ºèª
                5. **ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—**: Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—
                6. **ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**: å‡¦ç†å¯¾è±¡ãƒšãƒ¼ã‚¸ã‚’ç¢ºèªãƒ»é¸æŠ
                7. **ä¿å­˜**: Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                """)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®è¡¨ç¤º
                if 'uploaded_file' in st.session_state:
                    uploaded_file = st.session_state['uploaded_file']
                    st.info(f"ğŸ“ é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«: {uploaded_file.name}")
                
                if 'output_directory' in st.session_state:
                    output_dir = st.session_state['output_directory']
                    st.info(f"ğŸ“‚ ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
                
        else:
            st.markdown("""
            **è¨­å®šãŒå¿…è¦ãªé …ç›®:**
            1. **ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: bookmarks.htmlãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            2. **ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé¸æŠ**: Obsidianãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆã‚’æŒ‡å®š
            
            **ä»Šå¾Œã®å‡¦ç†æ‰‹é †:**
            3. **ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æ**: ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã¨URLã‚’è§£æ
            4. **é‡è¤‡ãƒã‚§ãƒƒã‚¯**: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®é‡è¤‡ã‚’ç¢ºèª
            5. **ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—**: Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—
            6. **ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**: å‡¦ç†å¯¾è±¡ãƒšãƒ¼ã‚¸ã‚’ç¢ºèªãƒ»é¸æŠ
            7. **ä¿å­˜**: Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            """)
            
            st.warning("ğŸ‘ˆ å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã‚’å®Œäº†ã—ã¦ãã ã•ã„")
    
    with col2:
        st.header("ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")
        
        # è¨­å®šçŠ¶æ³ã®è¡¨ç¤º
        file_validated = st.session_state.get('file_validated', False)
        dir_validated = st.session_state.get('directory_validated', False)
        
        if file_validated and dir_validated:
            st.success("âœ… è¨­å®šå®Œäº†")
            st.info("ğŸš€ è§£ææº–å‚™å®Œäº†")
        elif file_validated or dir_validated:
            st.warning("âš ï¸ è¨­å®šé€”ä¸­")
            st.info("ğŸ“‹ è¨­å®šã‚’å®Œäº†ã—ã¦ãã ã•ã„")
        else:
            st.info("ğŸ“‹ è¨­å®šå¾…ã¡")
            st.info("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã—ã¦ãã ã•ã„")
        
        # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
        if 'bookmarks' in st.session_state and 'directory_manager' in st.session_state:
            bookmarks = st.session_state['bookmarks']
            directory_manager = st.session_state['directory_manager']
            
            # å‡¦ç†å¯¾è±¡ã¨é™¤å¤–å¯¾è±¡ã‚’è¨ˆç®—
            total_bookmarks = len(bookmarks)
            excluded_count = sum(1 for bookmark in bookmarks if directory_manager.is_duplicate(bookmark))
            process_count = total_bookmarks - excluded_count
            
            st.metric("å‡¦ç†å¯¾è±¡ãƒšãƒ¼ã‚¸", process_count)
            st.metric("é™¤å¤–ãƒšãƒ¼ã‚¸", excluded_count)
            st.metric("å®Œäº†ãƒšãƒ¼ã‚¸", "0")  # ä»Šå¾Œã®å®Ÿè£…ã§æ›´æ–°
        elif 'bookmarks' in st.session_state:
            bookmarks = st.session_state['bookmarks']
            st.metric("å‡¦ç†å¯¾è±¡ãƒšãƒ¼ã‚¸", len(bookmarks))
            st.metric("é™¤å¤–ãƒšãƒ¼ã‚¸", "0")
            st.metric("å®Œäº†ãƒšãƒ¼ã‚¸", "0")
        else:
            st.metric("å‡¦ç†å¯¾è±¡ãƒšãƒ¼ã‚¸", "0")
            st.metric("é™¤å¤–ãƒšãƒ¼ã‚¸", "0")
            st.metric("å®Œäº†ãƒšãƒ¼ã‚¸", "0")
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666;'>
        <small>Bookmark to Obsidian Converter v1.0 | Streamlit Application</small>
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()