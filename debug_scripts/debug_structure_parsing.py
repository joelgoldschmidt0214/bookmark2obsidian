#!/usr/bin/env python3
"""
ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ§‹é€ è§£æã®è©³ç´°ãƒ‡ãƒãƒƒã‚°
ãªãœ4747å€‹ã®Aã‚¿ã‚°ã‹ã‚‰122å€‹ã—ã‹æŠ½å‡ºã•ã‚Œãªã„ã‹ã‚’èª¿æŸ»
"""

import os
import sys
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# app.pyã‹ã‚‰å¿…è¦ãªã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(".")
from app import BookmarkParser, Bookmark


def analyze_bookmark_structure(html_file_path):
    """ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ§‹é€ è§£æã®è©³ç´°åˆ†æ"""

    print(f"ğŸ“ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ§‹é€ è§£æã®è©³ç´°åˆ†æ: {html_file_path}")

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open(html_file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # BeautifulSoupã§è§£æ
    soup = BeautifulSoup(content, "html.parser")

    # å…¨Aã‚¿ã‚°ã‚’å–å¾—
    all_a_tags = soup.find_all("a")
    print(f"ğŸ” å…¨Aã‚¿ã‚°æ•°: {len(all_a_tags)}")

    # ãƒ«ãƒ¼ãƒˆDLã‚’å–å¾—
    root_dl = soup.find("dl")
    if not root_dl:
        print("âŒ ãƒ«ãƒ¼ãƒˆDLã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    print("âœ… ãƒ«ãƒ¼ãƒˆDLã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆç™ºè¦‹")

    # BookmarkParserã§è§£æ
    parser = BookmarkParser()
    bookmarks = parser.parse_bookmarks(content)
    print(f"ğŸ“š BookmarkParserã§è§£æã•ã‚ŒãŸãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°: {len(bookmarks)}")

    print("\n" + "=" * 80)
    print("æ§‹é€ è§£æã®è©³ç´°")
    print("=" * 80)

    # DLæ§‹é€ ã‚’è©³ã—ãåˆ†æ
    analyze_dl_structure_detailed(root_dl, parser)

    # å®Ÿéš›ã«å‡¦ç†ã•ã‚Œã‚‹Aã‚¿ã‚°ã¨å‡¦ç†ã•ã‚Œãªã„Aã‚¿ã‚°ã‚’æ¯”è¼ƒ
    print("\n" + "=" * 80)
    print("å‡¦ç†ã•ã‚Œã‚‹Aã‚¿ã‚° vs å‡¦ç†ã•ã‚Œãªã„Aã‚¿ã‚°ã®æ¯”è¼ƒ")
    print("=" * 80)

    compare_processed_vs_unprocessed(soup, parser)


def analyze_dl_structure_detailed(dl_element, parser, level=0, max_level=3):
    """DLæ§‹é€ ã®è©³ç´°åˆ†æï¼ˆåˆ¶é™ä»˜ãï¼‰"""
    if level > max_level:
        return

    indent = "  " * level

    # ã“ã®DLãƒ¬ãƒ™ãƒ«ã®DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
    all_dt_in_dl = dl_element.find_all("dt")

    # ãƒã‚¹ãƒˆã—ãŸDLå†…ã®DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã‚’é™¤å¤–
    nested_dls = dl_element.find_all("dl")[1:]  # æœ€åˆã®DLã¯è‡ªåˆ†è‡ªèº«ãªã®ã§é™¤å¤–
    nested_dt_elements = set()
    for nested_dl in nested_dls:
        nested_dt_elements.update(nested_dl.find_all("dt"))

    # ã“ã®DLãƒ¬ãƒ™ãƒ«ã®DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®ã¿ã‚’å‡¦ç†
    direct_dt_elements = [dt for dt in all_dt_in_dl if dt not in nested_dt_elements]

    print(f"{indent}ğŸ“ DLãƒ¬ãƒ™ãƒ« {level}:")
    print(f"{indent}  - å…¨DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆæ•°: {len(all_dt_in_dl)}")
    print(f"{indent}  - ãƒã‚¹ãƒˆã—ãŸDLæ•°: {len(nested_dls)}")
    print(f"{indent}  - ãƒã‚¹ãƒˆã—ãŸDTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆæ•°: {len(nested_dt_elements)}")
    print(f"{indent}  - ç›´æ¥ã®DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆæ•°: {len(direct_dt_elements)}")

    folder_count = 0
    bookmark_count = 0
    excluded_count = 0

    for i, dt in enumerate(direct_dt_elements):
        if i >= 10 and level == 0:  # ãƒ«ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«ã§ã¯æœ€åˆã®10å€‹ã ã‘è©³ç´°è¡¨ç¤º
            print(f"{indent}  ... (æ®‹ã‚Š{len(direct_dt_elements) - i}å€‹ã®DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆ)")
            break

        next_sibling = dt.find_next_sibling()

        if next_sibling and next_sibling.name == "dd":
            # ãƒ•ã‚©ãƒ«ãƒ€
            h3 = dt.find("h3")
            if h3:
                folder_name = h3.get_text(strip=True)
                folder_count += 1
                print(f"{indent}  ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€ {folder_count}: {folder_name}")

                # å†å¸°çš„ã«å‡¦ç†
                nested_dl = next_sibling.find("dl")
                if nested_dl:
                    analyze_dl_structure_detailed(
                        nested_dl, parser, level + 1, max_level
                    )
        else:
            # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯å€™è£œ
            a_tag = dt.find("a")
            if a_tag:
                url = a_tag.get("href", "").strip()
                title = a_tag.get_text(strip=True)

                if url and title:
                    temp_bookmark = Bookmark(title=title, url=url, folder_path=[])

                    if parser._should_exclude_bookmark(temp_bookmark):
                        excluded_count += 1
                        if excluded_count <= 3:  # æœ€åˆã®3å€‹ã ã‘è¡¨ç¤º
                            reason = get_exclusion_reason(parser, temp_bookmark)
                            print(
                                f"{indent}  âŒ é™¤å¤– {excluded_count}: {title[:30]}... ({reason})"
                            )
                    else:
                        bookmark_count += 1
                        if bookmark_count <= 3:  # æœ€åˆã®3å€‹ã ã‘è¡¨ç¤º
                            print(
                                f"{indent}  âœ… æœ‰åŠ¹ {bookmark_count}: {title[:30]}..."
                            )

    print(
        f"{indent}ğŸ“Š ãƒ¬ãƒ™ãƒ« {level} é›†è¨ˆ: ãƒ•ã‚©ãƒ«ãƒ€={folder_count}, æœ‰åŠ¹ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯={bookmark_count}, é™¤å¤–={excluded_count}"
    )


def compare_processed_vs_unprocessed(soup, parser):
    """å‡¦ç†ã•ã‚Œã‚‹Aã‚¿ã‚°ã¨å‡¦ç†ã•ã‚Œãªã„Aã‚¿ã‚°ã®æ¯”è¼ƒ"""

    # BookmarkParserã§å®Ÿéš›ã«å‡¦ç†ã•ã‚Œã‚‹Aã‚¿ã‚°ã‚’ç‰¹å®š
    processed_a_tags = set()

    # ãƒ«ãƒ¼ãƒˆDLã‹ã‚‰é–‹å§‹ã—ã¦ã€å®Ÿéš›ã«å‡¦ç†ã•ã‚Œã‚‹Aã‚¿ã‚°ã‚’è¿½è·¡
    root_dl = soup.find("dl")
    if root_dl:
        track_processed_a_tags(root_dl, processed_a_tags)

    # å…¨Aã‚¿ã‚°ã‚’å–å¾—
    all_a_tags = soup.find_all("a")

    print("ğŸ“Š Aã‚¿ã‚°å‡¦ç†çŠ¶æ³:")
    print(f"  - å…¨Aã‚¿ã‚°æ•°: {len(all_a_tags)}")
    print(f"  - æ§‹é€ è§£æã§å‡¦ç†ã•ã‚Œã‚‹Aã‚¿ã‚°æ•°: {len(processed_a_tags)}")
    print(
        f"  - æ§‹é€ è§£æã§å‡¦ç†ã•ã‚Œãªã„Aã‚¿ã‚°æ•°: {len(all_a_tags) - len(processed_a_tags)}"
    )

    # å‡¦ç†ã•ã‚Œãªã„Aã‚¿ã‚°ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
    unprocessed_a_tags = [a for a in all_a_tags if a not in processed_a_tags]

    print("\nğŸ” æ§‹é€ è§£æã§å‡¦ç†ã•ã‚Œãªã„Aã‚¿ã‚°ã®ã‚µãƒ³ãƒ—ãƒ« (æœ€åˆã®20å€‹):")
    for i, a_tag in enumerate(unprocessed_a_tags[:20]):
        url = a_tag.get("href", "").strip()
        title = a_tag.get_text(strip=True)

        # è¦ªè¦ç´ ã®æƒ…å ±ã‚’å–å¾—
        parent_info = get_parent_info(a_tag)

        print(f"  {i + 1:2d}. {title[:40]}...")
        print(f"      URL: {url[:60]}...")
        print(f"      è¦ªè¦ç´ : {parent_info}")

    # å‡¦ç†ã•ã‚Œã‚‹Aã‚¿ã‚°ã®ã‚µãƒ³ãƒ—ãƒ«ã‚‚è¡¨ç¤º
    print("\nâœ… æ§‹é€ è§£æã§å‡¦ç†ã•ã‚Œã‚‹Aã‚¿ã‚°ã®ã‚µãƒ³ãƒ—ãƒ« (æœ€åˆã®10å€‹):")
    for i, a_tag in enumerate(list(processed_a_tags)[:10]):
        url = a_tag.get("href", "").strip()
        title = a_tag.get_text(strip=True)

        print(f"  {i + 1:2d}. {title[:40]}...")
        print(f"      URL: {url[:60]}...")


def track_processed_a_tags(dl_element, processed_a_tags):
    """DLæ§‹é€ ã§å®Ÿéš›ã«å‡¦ç†ã•ã‚Œã‚‹Aã‚¿ã‚°ã‚’è¿½è·¡"""

    # ã“ã®DLãƒ¬ãƒ™ãƒ«ã®DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
    all_dt_in_dl = dl_element.find_all("dt")

    # ãƒã‚¹ãƒˆã—ãŸDLå†…ã®DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã‚’é™¤å¤–
    nested_dls = dl_element.find_all("dl")[1:]  # æœ€åˆã®DLã¯è‡ªåˆ†è‡ªèº«ãªã®ã§é™¤å¤–
    nested_dt_elements = set()
    for nested_dl in nested_dls:
        nested_dt_elements.update(nested_dl.find_all("dt"))

    # ã“ã®DLãƒ¬ãƒ™ãƒ«ã®DTã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã®ã¿ã‚’å‡¦ç†
    direct_dt_elements = [dt for dt in all_dt_in_dl if dt not in nested_dt_elements]

    for dt in direct_dt_elements:
        next_sibling = dt.find_next_sibling()

        if next_sibling and next_sibling.name == "dd":
            # ãƒ•ã‚©ãƒ«ãƒ€ - å†å¸°çš„ã«å‡¦ç†
            nested_dl = next_sibling.find("dl")
            if nested_dl:
                track_processed_a_tags(nested_dl, processed_a_tags)
        else:
            # ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯
            a_tag = dt.find("a")
            if a_tag:
                processed_a_tags.add(a_tag)


def get_parent_info(a_tag):
    """Aã‚¿ã‚°ã®è¦ªè¦ç´ æƒ…å ±ã‚’å–å¾—"""
    parent = a_tag.parent
    if parent:
        parent_name = parent.name
        parent_class = parent.get("class", [])
        return f"{parent_name}" + (f".{'.'.join(parent_class)}" if parent_class else "")
    return "ä¸æ˜"


def get_exclusion_reason(parser, bookmark):
    """é™¤å¤–ç†ç”±ã‚’ç‰¹å®š"""
    if parser._is_domain_root_url(bookmark.url):
        return "ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒˆ"
    if not parser._is_valid_url(bookmark.url):
        return "ç„¡åŠ¹URL"
    if bookmark.url in parser.excluded_urls:
        return "é™¤å¤–URL"

    try:
        parsed_url = urlparse(bookmark.url)
        domain = parsed_url.netloc.lower()
        if domain in parser.excluded_domains:
            return "é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³"
    except:
        return "URLè§£æã‚¨ãƒ©ãƒ¼"

    return "ä¸æ˜"


if __name__ == "__main__":
    html_file = "bookmarks_2025_09_06.html"

    if not os.path.exists(html_file):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {html_file}")
        sys.exit(1)

    analyze_bookmark_structure(html_file)
