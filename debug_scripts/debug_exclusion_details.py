#!/usr/bin/env python3
"""
ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯é™¤å¤–ã®è©³ç´°åˆ†æ
ãªãœ97.4%ã‚‚ã®Aã‚¿ã‚°ãŒé™¤å¤–ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’èª¿æŸ»
"""

import os
import sys
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from collections import Counter

# app.pyã‹ã‚‰å¿…è¦ãªã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(".")
from app import BookmarkParser, Bookmark


def detailed_exclusion_analysis(html_file_path):
    """é™¤å¤–ç†ç”±ã®è©³ç´°åˆ†æ"""

    print(f"ğŸ“ é™¤å¤–ç†ç”±è©³ç´°åˆ†æ: {html_file_path}")

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open(html_file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # BeautifulSoupã§è§£æ
    soup = BeautifulSoup(content, "html.parser")

    # å…¨Aã‚¿ã‚°ã‚’å–å¾—
    all_a_tags = soup.find_all("a")
    print(f"ğŸ” å…¨Aã‚¿ã‚°æ•°: {len(all_a_tags)}")

    # é™¤å¤–ç†ç”±ã®çµ±è¨ˆ
    exclusion_reasons = Counter()
    valid_count = 0

    parser = BookmarkParser()

    print("\nğŸ“Š é™¤å¤–ç†ç”±åˆ†æ:")
    print("=" * 60)

    for i, a_tag in enumerate(all_a_tags):
        url = a_tag.get("href", "").strip()
        title = a_tag.get_text(strip=True)

        if not url:
            exclusion_reasons["URLãªã—"] += 1
            continue

        if not title:
            exclusion_reasons["ã‚¿ã‚¤ãƒˆãƒ«ãªã—"] += 1
            continue

        # ä»®ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        temp_bookmark = Bookmark(title=title, url=url, folder_path=[])

        # é™¤å¤–ãƒã‚§ãƒƒã‚¯
        if parser._is_domain_root_url(temp_bookmark.url):
            exclusion_reasons["ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒˆ"] += 1
        elif not parser._is_valid_url(temp_bookmark.url):
            exclusion_reasons["ç„¡åŠ¹URL"] += 1
        elif temp_bookmark.url in parser.excluded_urls:
            exclusion_reasons["é™¤å¤–URL"] += 1
        else:
            try:
                parsed_url = urlparse(temp_bookmark.url)
                domain = parsed_url.netloc.lower()
                if domain in parser.excluded_domains:
                    exclusion_reasons["é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³"] += 1
                else:
                    valid_count += 1
            except:
                exclusion_reasons["URLè§£æã‚¨ãƒ©ãƒ¼"] += 1

    # çµæœè¡¨ç¤º
    print(f"âœ… æœ‰åŠ¹ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: {valid_count}")
    print(f"âŒ é™¤å¤–ã•ã‚ŒãŸãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯: {len(all_a_tags) - valid_count}")
    print(
        f"ğŸ“Š é™¤å¤–ç‡: {((len(all_a_tags) - valid_count) / len(all_a_tags) * 100):.1f}%"
    )

    print("\nğŸ“‹ é™¤å¤–ç†ç”±åˆ¥çµ±è¨ˆ:")
    for reason, count in exclusion_reasons.most_common():
        percentage = (count / len(all_a_tags)) * 100
        print(f"  {reason}: {count:,}ä»¶ ({percentage:.1f}%)")

    # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒˆé™¤å¤–ã®è©³ç´°åˆ†æ
    print("\nğŸ” ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒˆé™¤å¤–ã®è©³ç´°åˆ†æ:")
    print("=" * 60)

    domain_root_samples = []
    for a_tag in all_a_tags:
        url = a_tag.get("href", "").strip()
        title = a_tag.get_text(strip=True)

        if url and title:
            temp_bookmark = Bookmark(title=title, url=url, folder_path=[])
            if parser._is_domain_root_url(temp_bookmark.url):
                domain_root_samples.append((title, url))
                if len(domain_root_samples) >= 20:
                    break

    print("ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒˆã¨ã—ã¦é™¤å¤–ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ« (æœ€åˆã®20ä»¶):")
    for i, (title, url) in enumerate(domain_root_samples, 1):
        print(f"  {i:2d}. {title[:40]}... â†’ {url[:60]}...")

    # ç„¡åŠ¹URLã®è©³ç´°åˆ†æ
    print("\nğŸ” ç„¡åŠ¹URLé™¤å¤–ã®è©³ç´°åˆ†æ:")
    print("=" * 60)

    invalid_url_samples = []
    for a_tag in all_a_tags:
        url = a_tag.get("href", "").strip()
        title = a_tag.get_text(strip=True)

        if url and title:
            temp_bookmark = Bookmark(title=title, url=url, folder_path=[])
            if not parser._is_valid_url(temp_bookmark.url):
                invalid_url_samples.append((title, url))
                if len(invalid_url_samples) >= 10:
                    break

    print("ç„¡åŠ¹URLã¨ã—ã¦é™¤å¤–ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ« (æœ€åˆã®10ä»¶):")
    for i, (title, url) in enumerate(invalid_url_samples, 1):
        print(f"  {i:2d}. {title[:40]}... â†’ {url[:60]}...")


def analyze_domain_root_logic():
    """ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒˆåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã®åˆ†æ"""
    print("\nğŸ”§ ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒˆåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã®åˆ†æ:")
    print("=" * 60)

    parser = BookmarkParser()

    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_urls = [
        "https://www.google.com/",
        "https://www.google.com",
        "https://www.google.com/search?q=test",
        "https://github.com/",
        "https://github.com",
        "https://github.com/user/repo",
        "https://example.com/page.html",
        "https://example.com/?param=value",
        "https://example.com/#section",
        "https://book.dmm.com/",
        "https://book.dmm.com",
        "https://book.dmm.com/detail/b123456789",
    ]

    print("ãƒ†ã‚¹ãƒˆURLåˆ¤å®šçµæœ:")
    for url in test_urls:
        is_root = parser._is_domain_root_url(url)
        parsed = urlparse(url)
        path = parsed.path.strip("/")
        print(f"  {'âœ…' if not is_root else 'âŒ'} {url}")
        print(
            f"      ãƒ‘ã‚¹: '{path}', ã‚¯ã‚¨ãƒª: '{parsed.query}', ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆ: '{parsed.fragment}'"
        )
        print(f"      ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒˆåˆ¤å®š: {is_root}")


if __name__ == "__main__":
    html_file = "bookmarks_2025_09_06.html"

    if not os.path.exists(html_file):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {html_file}")
        sys.exit(1)

    detailed_exclusion_analysis(html_file)
    analyze_domain_root_logic()
