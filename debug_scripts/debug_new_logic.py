#!/usr/bin/env python3
"""
æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯ã§ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯è§£æãƒ‡ãƒãƒƒã‚°
é‡è¤‡å‡¦ç†ã®ç¢ºèªã¨çµ±è¨ˆæƒ…å ±ã®å–å¾—
"""

import os
import sys
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from collections import Counter

# app.pyã‹ã‚‰å¿…è¦ãªã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(".")
from app import BookmarkParser, Bookmark


def analyze_new_logic(html_file_path):
    """æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯ã§ã®è§£æçµæœã‚’è©³ã—ãèª¿ã¹ã‚‹"""

    print(f"ğŸ“ æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯ã§ã®è§£æ: {html_file_path}")

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open(html_file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # BeautifulSoupã§è§£æ
    soup = BeautifulSoup(content, "html.parser")

    # å…¨Aã‚¿ã‚°ã‚’å–å¾—
    all_a_tags = soup.find_all("a")
    print(f"ğŸ” å…¨Aã‚¿ã‚°æ•°: {len(all_a_tags)}")

    # BookmarkParserã§è§£æ
    parser = BookmarkParser()
    bookmarks = parser.parse_bookmarks(content)
    print(f"ğŸ“š è§£æã•ã‚ŒãŸãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°: {len(bookmarks)}")

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    urls = [b.url for b in bookmarks]
    url_counts = Counter(urls)
    duplicates = {url: count for url, count in url_counts.items() if count > 1}

    print("\nğŸ“Š é‡è¤‡åˆ†æ:")
    print(f"  - ãƒ¦ãƒ‹ãƒ¼ã‚¯URLæ•°: {len(url_counts)}")
    print(f"  - é‡è¤‡URLæ•°: {len(duplicates)}")
    print(
        f"  - é‡è¤‡ã«ã‚ˆã‚‹ä½™åˆ†ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°: {sum(duplicates.values()) - len(duplicates)}"
    )

    if duplicates:
        print("\nğŸ” é‡è¤‡URLã‚µãƒ³ãƒ—ãƒ« (ä¸Šä½10å€‹):")
        for i, (url, count) in enumerate(
            sorted(duplicates.items(), key=lambda x: x[1], reverse=True)[:10]
        ):
            print(f"  {i + 1:2d}. {count}å›: {url[:80]}...")

    # ãƒ•ã‚©ãƒ«ãƒ€åˆ¥çµ±è¨ˆ
    folder_stats = Counter()
    for bookmark in bookmarks:
        folder_path = (
            "/".join(bookmark.folder_path) if bookmark.folder_path else "(ãƒ«ãƒ¼ãƒˆ)"
        )
        folder_stats[folder_path] += 1

    print("\nğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€åˆ¥çµ±è¨ˆ (ä¸Šä½20å€‹):")
    for i, (folder, count) in enumerate(folder_stats.most_common(20)):
        print(f"  {i + 1:2d}. {count:4d}å€‹: {folder}")

    # é™¤å¤–çµ±è¨ˆ
    excluded_count = 0
    exclusion_reasons = Counter()

    for a_tag in all_a_tags:
        url = a_tag.get("href", "").strip()
        title = a_tag.get_text(strip=True)

        if url and title:
            temp_bookmark = Bookmark(title=title, url=url, folder_path=[])
            if parser._should_exclude_bookmark(temp_bookmark):
                excluded_count += 1
                reason = get_exclusion_reason(parser, temp_bookmark)
                exclusion_reasons[reason] += 1

    print("\nâŒ é™¤å¤–çµ±è¨ˆ:")
    print(f"  - é™¤å¤–ã•ã‚ŒãŸAã‚¿ã‚°æ•°: {excluded_count}")
    for reason, count in exclusion_reasons.most_common():
        print(f"  - {reason}: {count}å€‹")

    # å‡¦ç†åŠ¹ç‡ã®è¨ˆç®—
    processed_rate = (len(bookmarks) / len(all_a_tags)) * 100 if all_a_tags else 0
    unique_rate = (len(url_counts) / len(all_a_tags)) * 100 if all_a_tags else 0

    print("\nğŸ“ˆ å‡¦ç†åŠ¹ç‡:")
    print(f"  - å‡¦ç†ç‡: {processed_rate:.1f}% ({len(bookmarks)}/{len(all_a_tags)})")
    print(f"  - ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡: {unique_rate:.1f}% ({len(url_counts)}/{len(all_a_tags)})")


def get_exclusion_reason(parser, bookmark):
    """é™¤å¤–ç†ç”±ã‚’ç‰¹å®š"""
    if parser._is_domain_root_url(bookmark.url):
        return "ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒˆ"
    if not parser._is_valid_url(bookmark.url):
        return "ç„¡åŠ¹URL"
    if bookmark.url in parser.excluded_urls:
        return "é™¤å¤–URL"

    try:
        parsed_url = urlparse(bookmark.url)
        domain = parsed_url.netloc.lower()
        if domain in parser.excluded_domains:
            return "é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³"
    except:
        return "URLè§£æã‚¨ãƒ©ãƒ¼"

    return "ä¸æ˜"


def check_duplicate_processing():
    """é‡è¤‡å‡¦ç†ã®åŸå› ã‚’èª¿æŸ»"""
    print("\nğŸ” é‡è¤‡å‡¦ç†ã®åŸå› èª¿æŸ»:")

    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§ç¢ºèª
    test_html = """
    <dl>
        <dt><h3>ãƒ•ã‚©ãƒ«ãƒ€1</h3>
            <dl>
                <dt><a href="https://example.com/1">ãƒ†ã‚¹ãƒˆ1</a></dt>
                <dt><a href="https://example.com/2">ãƒ†ã‚¹ãƒˆ2</a></dt>
            </dl>
        </dt>
        <dt><a href="https://example.com/3">ãƒ†ã‚¹ãƒˆ3</a></dt>
    </dl>
    """

    parser = BookmarkParser()
    bookmarks = parser.parse_bookmarks(test_html)

    print("  ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹çµæœ:")
    print("  - æœŸå¾…ã•ã‚Œã‚‹ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°: 3")
    print(f"  - å®Ÿéš›ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°: {len(bookmarks)}")

    for i, bookmark in enumerate(bookmarks):
        folder_path = (
            "/".join(bookmark.folder_path) if bookmark.folder_path else "(ãƒ«ãƒ¼ãƒˆ)"
        )
        print(f"    {i + 1}. {bookmark.title} (ãƒ•ã‚©ãƒ«ãƒ€: {folder_path})")


if __name__ == "__main__":
    html_file = "bookmarks_2025_09_06.html"

    if not os.path.exists(html_file):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {html_file}")
        sys.exit(1)

    analyze_new_logic(html_file)
    check_duplicate_processing()
